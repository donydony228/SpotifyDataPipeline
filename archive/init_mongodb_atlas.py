from pymongo import MongoClient
from pymongo.server_api import ServerApi
from datetime import datetime
import os
from dotenv import load_dotenv

load_dotenv()

def init_mongodb_atlas():
    try:
        client = MongoClient(
            os.getenv('MONGODB_ATLAS_URL'),
            server_api=ServerApi('1')
        )
        
        db = client[os.getenv('MONGODB_ATLAS_DB_NAME', 'job_market_data')]
        
        print("ğŸš€ åˆå§‹åŒ– MongoDB Atlas è³‡æ–™åº«...")
        
        # 1. å»ºç«‹ raw_jobs_data é›†åˆä¸¦è¨­å®šé©—è­‰è¦å‰‡
        print("ğŸ“ å»ºç«‹ raw_jobs_data é›†åˆ...")
        
        # æª¢æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
        if 'raw_jobs_data' not in db.list_collection_names():
            db.create_collection('raw_jobs_data', validator={
                "$jsonSchema": {
                    "bsonType": "object",
                    "required": ["source", "job_data", "metadata"],
                    "properties": {
                        "source": {
                            "bsonType": "string",
                            "enum": ["linkedin", "indeed", "glassdoor", "angellist"]
                        },
                        "job_data": {
                            "bsonType": "object"
                        },
                        "metadata": {
                            "bsonType": "object",
                            "required": ["scraped_at", "batch_id"],
                            "properties": {
                                "scraped_at": {"bsonType": "date"},
                                "batch_id": {"bsonType": "string"},
                                "scraper_version": {"bsonType": "string"},
                                "source_url": {"bsonType": "string"}
                            }
                        }
                    }
                }
            })
            print("  âœ… raw_jobs_data é›†åˆå·²å»ºç«‹")
        else:
            print("  âš ï¸  raw_jobs_data é›†åˆå·²å­˜åœ¨")
        
        # 2. å»ºç«‹å…¶ä»–é›†åˆ
        collections_to_create = [
            'data_quality_reports',
            'scraper_logs',
            'batch_metadata'
        ]
        
        for collection_name in collections_to_create:
            if collection_name not in db.list_collection_names():
                db.create_collection(collection_name)
                print(f"  âœ… {collection_name} é›†åˆå·²å»ºç«‹")
            else:
                print(f"  âš ï¸  {collection_name} é›†åˆå·²å­˜åœ¨")
        
        # 3. å»ºç«‹ç´¢å¼•æå‡æŸ¥è©¢æ•ˆèƒ½
        print("\nğŸ” å»ºç«‹ç´¢å¼•...")
        
        raw_jobs_collection = db['raw_jobs_data']
        
        # å»ºç«‹è¤‡åˆç´¢å¼•
        indexes_to_create = [
            ([("source", 1), ("metadata.scraped_at", -1)], "source_scraped_idx"),
            ([("metadata.batch_id", 1)], "batch_id_idx"),
            ([("job_data.job_id", 1), ("source", 1)], "job_id_source_idx")
        ]
        
        existing_indexes = [idx['name'] for idx in raw_jobs_collection.list_indexes()]
        
        for index_fields, index_name in indexes_to_create:
            if index_name not in existing_indexes:
                raw_jobs_collection.create_index(index_fields, name=index_name, unique=(index_name == "job_id_source_idx"))
                print(f"  âœ… ç´¢å¼• {index_name} å·²å»ºç«‹")
            else:
                print(f"  âš ï¸  ç´¢å¼• {index_name} å·²å­˜åœ¨")
        
        # 4. æ’å…¥æ¸¬è©¦è³‡æ–™
        print("\nğŸ“Š æ’å…¥æ¸¬è©¦è³‡æ–™...")
        
        test_data = {
            "source": "linkedin",
            "job_data": {
                "job_id": "atlas_test_job_001",
                "job_title": "Senior Data Engineer - Atlas Test",
                "company": "Tech Corp",
                "location": "San Francisco, CA",
                "salary": "$120,000 - $180,000",
                "description": "Looking for a Senior Data Engineer to join our growing team...",
                "skills": ["Python", "SQL", "AWS", "Docker", "Apache Airflow"],
                "employment_type": "Full-time",
                "work_arrangement": "Hybrid"
            },
            "metadata": {
                "scraped_at": datetime.utcnow(),
                "batch_id": f"atlas_test_batch_{datetime.now().strftime('%Y%m%d')}",
                "scraper_version": "1.0.0",
                "source_url": "https://linkedin.com/jobs/atlas_test_job_001"
            },
            "data_quality": {
                "completeness_score": 0.98,
                "flags": []
            }
        }
        
        # æ’å…¥æ¸¬è©¦è³‡æ–™ï¼ˆé¿å…é‡è¤‡ï¼‰
        existing_test = raw_jobs_collection.find_one({
            "job_data.job_id": "atlas_test_job_001",
            "source": "linkedin"
        })
        
        if not existing_test:
            result = raw_jobs_collection.insert_one(test_data)
            print(f"  âœ… æ¸¬è©¦è³‡æ–™å·²æ’å…¥ï¼ŒID: {result.inserted_id}")
        else:
            print("  âš ï¸  æ¸¬è©¦è³‡æ–™å·²å­˜åœ¨")
        
        # 5. é©—è­‰è¨­å®š
        print("\nğŸ” é©—è­‰ Atlas è¨­å®š...")
        
        # æª¢æŸ¥é›†åˆæ•¸é‡
        collections = db.list_collection_names()
        print(f"  ğŸ“‚ é›†åˆç¸½æ•¸: {len(collections)}")
        for collection in collections:
            count = db[collection].count_documents({})
            print(f"    - {collection}: {count} ç­†æ–‡æª”")
        
        # æ¸¬è©¦æŸ¥è©¢
        test_query = raw_jobs_collection.find_one({"source": "linkedin"})
        if test_query:
            print("  âœ… æŸ¥è©¢æ¸¬è©¦é€šé")
            print(f"    ç¯„ä¾‹è³‡æ–™: {test_query['job_data']['job_title']}")
        
        client.close()
        print("\nğŸ‰ MongoDB Atlas åˆå§‹åŒ–å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ MongoDB Atlas åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        return False

if __name__ == "__main__":
    init_mongodb_atlas()