# dags/scrapers/linkedin_scraper_dag.py
# LinkedIn æ¯æ—¥çˆ¬èŸ² DAG - Phase 1 åŸºç¤æ¡†æ¶

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import sys
import os

# åŠ å…¥ src è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../src'))

# ============================================================================
# DAG é…ç½®
# ============================================================================

default_args = {
    'owner': 'data-engineering-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
    'execution_timeout': timedelta(hours=2)  # é˜²æ­¢çˆ¬èŸ²å¡ä½
}

dag = DAG(
    'linkedin_daily_scraper',
    default_args=default_args,
    description='LinkedIn è·ç¼ºæ¯æ—¥çˆ¬èŸ² - å®Œæ•´æ•¸æ“šæ”¶é›†æµç¨‹',
    schedule='@daily',  # æ¯å¤©åŸ·è¡Œä¸€æ¬¡
    max_active_runs=1,  # é¿å…é‡ç–ŠåŸ·è¡Œ
    catchup=False,      # ä¸å›è£œæ­·å²åŸ·è¡Œ
    tags=['scraper', 'linkedin', 'daily', 'jobs']
)

# ============================================================================
# Task å‡½æ•¸å®šç¾© (Phase 1 - åŸºç¤ç‰ˆæœ¬)
# ============================================================================

def setup_scraper_config(**context):
    """è¨­å®šä»Šæ—¥çˆ¬èŸ²é…ç½®åƒæ•¸"""
    from datetime import datetime
    
    # å‹•æ…‹ç”Ÿæˆæ‰¹æ¬¡ ID
    execution_date = context['ds']  # YYYY-MM-DD æ ¼å¼
    batch_id = f"linkedin_{execution_date.replace('-', '')}"
    
    config = {
        'batch_id': batch_id,
        'execution_date': execution_date,
        
        # çˆ¬å–ç›®æ¨™è¨­å®š
        'target_jobs': 100,  # Phase 1 å…ˆè¨­å°‘ä¸€é»æ¸¬è©¦
        'max_pages_per_search': 3,
        
        # æœå°‹æ¢ä»¶
        'search_terms': [
            'data engineer',
            'senior data engineer',
            'data scientist'
        ],
        'locations': [
            'San Francisco Bay Area',
            'New York',
            'Seattle'
        ],
        
        # åçˆ¬è¨­å®š
        'delay_range': (2, 4),  # éš¨æ©Ÿå»¶é² 2-4 ç§’
        'request_timeout': 30,
        'max_retries': 3,
        
        # å“è³ªæ§åˆ¶
        'min_required_fields': ['job_title', 'company_name', 'location'],
        'skip_duplicates': True
    }
    
    print(f"âœ… çˆ¬èŸ²é…ç½®å·²ç”Ÿæˆ:")
    print(f"   æ‰¹æ¬¡ ID: {config['batch_id']}")
    print(f"   ç›®æ¨™è·ç¼º: {config['target_jobs']}")
    print(f"   æœå°‹é—œéµå­—: {len(config['search_terms'])} å€‹")
    print(f"   ç›®æ¨™åŸå¸‚: {len(config['locations'])} å€‹")
    
    # å„²å­˜é…ç½®åˆ° XCom ä¾›å¾ŒçºŒ Task ä½¿ç”¨
    context['task_instance'].xcom_push(key='scraper_config', value=config)
    
    return f"Config ready for batch {config['batch_id']}"


def check_rate_limits(**context):
    """æª¢æŸ¥çˆ¬å–é »ç‡é™åˆ¶"""
    import redis
    from datetime import datetime, timedelta
    
    try:
        # é€£æ¥ Redis (æœ¬åœ°é–‹ç™¼ç’°å¢ƒ)
        r = redis.from_url('redis://localhost:6379')
        
        # æª¢æŸ¥ä¸Šæ¬¡ LinkedIn çˆ¬å–æ™‚é–“
        last_scrape_key = 'linkedin:last_scrape_time'
        last_scrape = r.get(last_scrape_key)
        
        current_time = datetime.now()
        
        if last_scrape:
            last_scrape_time = datetime.fromisoformat(last_scrape.decode())
            time_diff = current_time - last_scrape_time
            
            # æœ€å°‘é–“éš” 12 å°æ™‚ (ä¿å®ˆç­–ç•¥)
            min_interval = timedelta(hours=12)
            
            if time_diff < min_interval:
                remaining = min_interval - time_diff
                print(f"âš ï¸  çˆ¬å–é »ç‡é™åˆ¶: é‚„éœ€ç­‰å¾… {remaining}")
                print(f"   ä¸Šæ¬¡çˆ¬å–: {last_scrape_time}")
                # Phase 1 æš«æ™‚ä¸è·³éï¼Œåªè¨˜éŒ„è­¦å‘Š
                # raise AirflowSkipException("Rate limit hit")
        
        # æ›´æ–°çˆ¬å–æ™‚é–“è¨˜éŒ„
        r.set(last_scrape_key, current_time.isoformat(), ex=86400)  # 24å°æ™‚éæœŸ
        
        print(f"âœ… é »ç‡æª¢æŸ¥é€šéï¼Œå¯ä»¥é–‹å§‹çˆ¬å–")
        return "Rate limit check passed"
        
    except Exception as e:
        print(f"âš ï¸  Redis é€£ç·šå¤±æ•—ï¼Œè·³éé »ç‡æª¢æŸ¥: {str(e)}")
        return "Rate limit check skipped (Redis unavailable)"


def scrape_linkedin_jobs(**context):
    """åŸ·è¡Œ LinkedIn è·ç¼ºçˆ¬èŸ² - Phase 1 åŸºç¤ç‰ˆæœ¬"""
    
    # å–å¾—é…ç½®
    config = context['task_instance'].xcom_pull(
        task_ids='setup_scraper_config', 
        key='scraper_config'
    )
    
    print(f"ğŸš€ é–‹å§‹çˆ¬å– LinkedIn è·ç¼º...")
    print(f"   æ‰¹æ¬¡ ID: {config['batch_id']}")
    
    try:
        # Phase 1: ä½¿ç”¨åŸºç¤çˆ¬èŸ²é‚è¼¯
        from scrapers.linkedin_scraper import LinkedInBasicScraper
        
        scraper = LinkedInBasicScraper(config)
        jobs_data = scraper.scrape_jobs()
        
        # çµ±è¨ˆçµæœ
        total_jobs = len(jobs_data)
        success_rate = scraper.get_success_rate()
        
        print(f"âœ… çˆ¬å–å®Œæˆ:")
        print(f"   ç¸½è¨ˆè·ç¼º: {total_jobs}")
        print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        
        # å„²å­˜çµæœåˆ° XCom
        result = {
            'batch_id': config['batch_id'],
            'jobs_data': jobs_data,
            'total_jobs': total_jobs,
            'success_rate': success_rate,
            'scrape_timestamp': datetime.now().isoformat()
        }
        
        context['task_instance'].xcom_push(key='scrape_result', value=result)
        
        return f"Successfully scraped {total_jobs} jobs"
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±æ•—: {str(e)}")
        raise


def validate_scraped_data(**context):
    """é©—è­‰çˆ¬å–è³‡æ–™å“è³ª"""
    
    # å–å¾—çˆ¬å–çµæœ
    scrape_result = context['task_instance'].xcom_pull(
        task_ids='scrape_linkedin_jobs',
        key='scrape_result'
    )
    
    if not scrape_result or not scrape_result.get('jobs_data'):
        raise ValueError("No scraped data found")
    
    jobs_data = scrape_result['jobs_data']
    
    print(f"ğŸ” é–‹å§‹é©—è­‰ {len(jobs_data)} ç­†è·ç¼ºè³‡æ–™...")
    
    # Phase 1 åŸºç¤é©—è­‰é‚è¼¯
    validation_results = {
        'total_jobs': len(jobs_data),
        'valid_jobs': 0,
        'invalid_jobs': 0,
        'completeness_scores': [],
        'quality_flags': []
    }
    
    valid_jobs = []
    
    for i, job in enumerate(jobs_data):
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_fields = ['job_title', 'company_name', 'location', 'job_url']
        missing_fields = [field for field in required_fields if not job.get(field)]
        
        # è¨ˆç®—å®Œæ•´æ€§åˆ†æ•¸
        total_fields = ['job_title', 'company_name', 'location', 'job_url', 
                       'job_description', 'salary_range', 'employment_type']
        filled_fields = sum(1 for field in total_fields if job.get(field))
        completeness_score = filled_fields / len(total_fields)
        
        validation_results['completeness_scores'].append(completeness_score)
        
        if missing_fields:
            validation_results['invalid_jobs'] += 1
            validation_results['quality_flags'].append({
                'job_index': i,
                'missing_fields': missing_fields,
                'completeness_score': completeness_score
            })
            print(f"âš ï¸  è·ç¼º {i+1} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_fields}")
        else:
            validation_results['valid_jobs'] += 1
            job['completeness_score'] = completeness_score
            valid_jobs.append(job)
    
    # è¨ˆç®—æ•´é«”å“è³ªæŒ‡æ¨™
    avg_completeness = sum(validation_results['completeness_scores']) / len(validation_results['completeness_scores'])
    validation_results['average_completeness'] = avg_completeness
    
    print(f"âœ… è³‡æ–™é©—è­‰å®Œæˆ:")
    print(f"   æœ‰æ•ˆè·ç¼º: {validation_results['valid_jobs']}")
    print(f"   ç„¡æ•ˆè·ç¼º: {validation_results['invalid_jobs']}")
    print(f"   å¹³å‡å®Œæ•´æ€§: {avg_completeness:.2%}")
    
    # æ›´æ–°çµæœ
    validated_result = scrape_result.copy()
    validated_result['jobs_data'] = valid_jobs
    validated_result['validation_results'] = validation_results
    
    context['task_instance'].xcom_push(key='validated_data', value=validated_result)
    
    return f"Validated {validation_results['valid_jobs']} valid jobs"


def store_to_mongodb(**context):
    """å„²å­˜è³‡æ–™åˆ° MongoDB Atlas"""
    
    # å–å¾—é©—è­‰å¾Œçš„è³‡æ–™
    validated_data = context['task_instance'].xcom_pull(
        task_ids='validate_scraped_data',
        key='validated_data'
    )
    
    if not validated_data or not validated_data.get('jobs_data'):
        print("âš ï¸  æ²’æœ‰æœ‰æ•ˆè³‡æ–™éœ€è¦å„²å­˜")
        return "No data to store"
    
    jobs_data = validated_data['jobs_data']
    batch_id = validated_data['batch_id']
    
    print(f"ğŸ’¾ é–‹å§‹å„²å­˜ {len(jobs_data)} ç­†è³‡æ–™åˆ° MongoDB Atlas...")
    
    try:
        from pymongo import MongoClient
        from pymongo.server_api import ServerApi
        import os
        
        # é€£æ¥ MongoDB Atlas
        mongodb_url = os.getenv('MONGODB_ATLAS_URL')
        if not mongodb_url:
            raise ValueError("MONGODB_ATLAS_URL environment variable not set")
        
        client = MongoClient(mongodb_url, server_api=ServerApi('1'))
        db = client[os.getenv('MONGODB_ATLAS_DB_NAME', 'job_market_data')]
        collection = db['raw_jobs_data']
        
        # æ‰¹æ¬¡æ’å…¥/æ›´æ–°è³‡æ–™
        operations = []
        for job in jobs_data:
            document = {
                'source': 'linkedin',
                'job_data': job,
                'metadata': {
                    'scraped_at': datetime.now(),
                    'batch_id': batch_id,
                    'scraper_version': '1.0.0-phase1',
                    'source_url': job.get('job_url', '')
                },
                'data_quality': {
                    'completeness_score': job.get('completeness_score', 0),
                    'flags': []
                }
            }
            
            # ä½¿ç”¨ job_url ä½œç‚ºå”¯ä¸€è­˜åˆ¥ï¼Œé¿å…é‡è¤‡
            filter_condition = {
                'job_data.job_url': job.get('job_url'),
                'source': 'linkedin'
            }
            
            operations.append({
                'filter': filter_condition,
                'document': document,
                'upsert': True
            })
        
        # åŸ·è¡Œæ‰¹æ¬¡ upsert
        if operations:
            results = []
            for op in operations:
                result = collection.replace_one(
                    op['filter'], 
                    op['document'], 
                    upsert=op['upsert']
                )
                results.append(result)
            
            inserted_count = sum(1 for r in results if r.upserted_id)
            updated_count = sum(1 for r in results if r.modified_count > 0)
            
            print(f"âœ… MongoDB å„²å­˜å®Œæˆ:")
            print(f"   æ–°å¢: {inserted_count} ç­†")
            print(f"   æ›´æ–°: {updated_count} ç­†")
            
            # è¨˜éŒ„å„²å­˜çµ±è¨ˆ
            storage_stats = {
                'mongodb_inserted': inserted_count,
                'mongodb_updated': updated_count,
                'mongodb_total': len(operations)
            }
            
            context['task_instance'].xcom_push(key='mongodb_stats', value=storage_stats)
            
            client.close()
            return f"Stored {len(operations)} jobs to MongoDB"
        
    except Exception as e:
        print(f"âŒ MongoDB å„²å­˜å¤±æ•—: {str(e)}")
        raise


def store_to_postgres_raw(**context):
    """å„²å­˜è³‡æ–™åˆ° PostgreSQL Raw Staging"""
    
    # å–å¾—é©—è­‰å¾Œçš„è³‡æ–™
    validated_data = context['task_instance'].xcom_pull(
        task_ids='validate_scraped_data',
        key='validated_data'
    )
    
    if not validated_data or not validated_data.get('jobs_data'):
        print("âš ï¸  æ²’æœ‰æœ‰æ•ˆè³‡æ–™éœ€è¦å„²å­˜")
        return "No data to store"
    
    jobs_data = validated_data['jobs_data']
    batch_id = validated_data['batch_id']
    
    print(f"ğŸ˜ é–‹å§‹å„²å­˜ {len(jobs_data)} ç­†è³‡æ–™åˆ° PostgreSQL Raw Staging...")
    
    try:
        import psycopg2
        import json
        import os
        
        # é€£æ¥ Supabase PostgreSQL
        supabase_url = os.getenv('SUPABASE_DB_URL')
        if not supabase_url:
            raise ValueError("SUPABASE_DB_URL environment variable not set")
        
        conn = psycopg2.connect(supabase_url)
        cur = conn.cursor()
        
        # æº–å‚™æ’å…¥è³‡æ–™
        insert_sql = """
        INSERT INTO raw_staging.linkedin_jobs_raw (
            source_job_id, source_url, job_title, company_name,
            location_raw, job_description, employment_type,
            work_arrangement, raw_json, batch_id, scraped_at
        ) VALUES (
            %(source_job_id)s, %(source_url)s, %(job_title)s, %(company_name)s,
            %(location_raw)s, %(job_description)s, %(employment_type)s,
            %(work_arrangement)s, %(raw_json)s, %(batch_id)s, %(scraped_at)s
        ) ON CONFLICT (source_job_id, batch_id) DO UPDATE SET
            job_title = EXCLUDED.job_title,
            company_name = EXCLUDED.company_name,
            location_raw = EXCLUDED.location_raw,
            job_description = EXCLUDED.job_description,
            raw_json = EXCLUDED.raw_json,
            scraped_at = EXCLUDED.scraped_at
        """
        
        inserted_count = 0
        for job in jobs_data:
            # å¾ job_url æå– job_id (LinkedIn URL çµæ§‹)
            job_url = job.get('job_url', '')
            job_id = 'unknown'
            if '/jobs/view/' in job_url:
                try:
                    job_id = job_url.split('/jobs/view/')[-1].split('?')[0]
                except:
                    job_id = f"batch_{batch_id}_{inserted_count}"
            
            row_data = {
                'source_job_id': job_id,
                'source_url': job_url,
                'job_title': job.get('job_title', ''),
                'company_name': job.get('company_name', ''),
                'location_raw': job.get('location', ''),
                'job_description': job.get('job_description', ''),
                'employment_type': job.get('employment_type', ''),
                'work_arrangement': job.get('work_arrangement', ''),
                'raw_json': json.dumps(job),
                'batch_id': batch_id,
                'scraped_at': datetime.now()
            }
            
            cur.execute(insert_sql, row_data)
            inserted_count += 1
        
        conn.commit()
        cur.close()
        conn.close()
        
        print(f"âœ… PostgreSQL å„²å­˜å®Œæˆ: {inserted_count} ç­†")
        
        # è¨˜éŒ„å„²å­˜çµ±è¨ˆ
        storage_stats = {
            'postgres_inserted': inserted_count
        }
        
        context['task_instance'].xcom_push(key='postgres_stats', value=storage_stats)
        
        return f"Stored {inserted_count} jobs to PostgreSQL"
        
    except Exception as e:
        print(f"âŒ PostgreSQL å„²å­˜å¤±æ•—: {str(e)}")
        raise


def log_scraping_metrics(**context):
    """è¨˜éŒ„çˆ¬å–æŒ‡æ¨™å’Œçµ±è¨ˆ"""
    
    # æ”¶é›†æ‰€æœ‰ Task çš„åŸ·è¡Œçµæœ
    scrape_result = context['task_instance'].xcom_pull(
        task_ids='scrape_linkedin_jobs',
        key='scrape_result'
    ) or {}
    
    validated_data = context['task_instance'].xcom_pull(
        task_ids='validate_scraped_data',
        key='validated_data'
    ) or {}
    
    mongodb_stats = context['task_instance'].xcom_pull(
        task_ids='store_to_mongodb',
        key='mongodb_stats'
    ) or {}
    
    postgres_stats = context['task_instance'].xcom_pull(
        task_ids='store_to_postgres_raw',
        key='postgres_stats'
    ) or {}
    
    # ç·¨è­¯å®Œæ•´çš„åŸ·è¡Œå ±å‘Š
    execution_report = {
        'dag_id': context['dag'].dag_id,
        'execution_date': context['ds'],
        'batch_id': scrape_result.get('batch_id', 'unknown'),
        
        # çˆ¬å–çµ±è¨ˆ
        'scraping': {
            'total_scraped': scrape_result.get('total_jobs', 0),
            'success_rate': scrape_result.get('success_rate', 0),
            'scrape_timestamp': scrape_result.get('scrape_timestamp')
        },
        
        # é©—è­‰çµ±è¨ˆ
        'validation': validated_data.get('validation_results', {}),
        
        # å„²å­˜çµ±è¨ˆ
        'storage': {
            'mongodb': mongodb_stats,
            'postgresql': postgres_stats
        },
        
        # åŸ·è¡Œæ™‚é–“
        'execution_time': {
            'start_time': context['task_instance'].start_date.isoformat() if context['task_instance'].start_date else None,
            'end_time': datetime.now().isoformat()
        }
    }
    
    print(f"ğŸ“Š çˆ¬å–åŸ·è¡Œå ±å‘Š:")
    print(f"=" * 50)
    print(f"æ‰¹æ¬¡ ID: {execution_report['batch_id']}")
    print(f"åŸ·è¡Œæ—¥æœŸ: {execution_report['execution_date']}")
    print(f"çˆ¬å–è·ç¼º: {execution_report['scraping']['total_scraped']}")
    print(f"æˆåŠŸç‡: {execution_report['scraping']['success_rate']:.1%}")
    print(f"æœ‰æ•ˆè·ç¼º: {execution_report['validation'].get('valid_jobs', 0)}")
    print(f"MongoDB å„²å­˜: {execution_report['storage']['mongodb'].get('mongodb_total', 0)}")
    print(f"PostgreSQL å„²å­˜: {execution_report['storage']['postgresql'].get('postgres_inserted', 0)}")
    
    # å„²å­˜å ±å‘Š (å¯ä»¥å¾ŒçºŒåŠ å…¥åˆ°è³‡æ–™åº«æˆ–æª”æ¡ˆ)
    context['task_instance'].xcom_push(key='execution_report', value=execution_report)
    
    return "Metrics logged successfully"


# ============================================================================
# Task å®šç¾©
# ============================================================================

# Task 1: è¨­å®šçˆ¬èŸ²é…ç½®
setup_config_task = PythonOperator(
    task_id='setup_scraper_config',
    python_callable=setup_scraper_config,
    dag=dag
)

# Task 2: æª¢æŸ¥é »ç‡é™åˆ¶
rate_limit_task = PythonOperator(
    task_id='check_rate_limits',
    python_callable=check_rate_limits,
    dag=dag
)

# Task 3: åŸ·è¡Œçˆ¬èŸ²
scrape_task = PythonOperator(
    task_id='scrape_linkedin_jobs',
    python_callable=scrape_linkedin_jobs,
    dag=dag
)

# Task 4: é©—è­‰è³‡æ–™
validate_task = PythonOperator(
    task_id='validate_scraped_data',
    python_callable=validate_scraped_data,
    dag=dag
)

# Task 5: å„²å­˜åˆ° MongoDB
mongodb_task = PythonOperator(
    task_id='store_to_mongodb',
    python_callable=store_to_mongodb,
    dag=dag
)

# Task 6: å„²å­˜åˆ° PostgreSQL
postgres_task = PythonOperator(
    task_id='store_to_postgres_raw',
    python_callable=store_to_postgres_raw,
    dag=dag
)

# Task 7: è¨˜éŒ„æŒ‡æ¨™
metrics_task = PythonOperator(
    task_id='log_scraping_metrics',
    python_callable=log_scraping_metrics,
    dag=dag
)

# ç³»çµ±æª¢æŸ¥ Task (å¯é¸)
system_check_task = BashOperator(
    task_id='system_check',
    bash_command='''
    echo "ğŸ–¥ï¸  LinkedIn çˆ¬èŸ²ç³»çµ±æª¢æŸ¥:"
    echo "åŸ·è¡Œæ™‚é–“: $(date)"
    echo "Python ç‰ˆæœ¬: $(python3 --version)"
    echo "å¯ç”¨è¨˜æ†¶é«”: $(free -h | grep Mem | awk '{print $7}')" || echo "è¨˜æ†¶é«”è³‡è¨Šä¸å¯ç”¨"
    ''',
    dag=dag
)

# ============================================================================
# Task ä¾è³´é—œä¿‚
# ============================================================================

# ç·šæ€§åŸ·è¡Œæµç¨‹
system_check_task >> setup_config_task >> rate_limit_task >> scrape_task >> validate_task

# ä¸¦è¡Œå„²å­˜ (MongoDB å’Œ PostgreSQL å¯åŒæ™‚é€²è¡Œ)
validate_task >> [mongodb_task, postgres_task]

# æœ€çµ‚æŒ‡æ¨™è¨˜éŒ„ (ç­‰å¾…æ‰€æœ‰å„²å­˜å®Œæˆ)
[mongodb_task, postgres_task] >> metrics_task