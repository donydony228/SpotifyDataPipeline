# src/scrapers/mock_linkedin_scraper.py
# æ¨¡æ“¬ LinkedIn çˆ¬èŸ² - ç”¨æ–¼æ¸¬è©¦ DAG æµç¨‹

import random
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
import uuid

class MockLinkedInScraper:
    """
    æ¨¡æ“¬ LinkedIn çˆ¬èŸ² - ä¸å¯¦éš›é€£æ¥ LinkedIn
    ç”Ÿæˆå‡è³‡æ–™ç”¨æ–¼æ¸¬è©¦å®Œæ•´çš„ ETL æµç¨‹
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.scraped_jobs = []
        self.success_count = 0
        self.total_attempts = 0
        
        # è¨­å®š logger
        self.logger = logging.getLogger(__name__)
        
        # æ¨¡æ“¬è³‡æ–™æ± 
        self.mock_data = self._create_mock_data_pools()
        
    def _create_mock_data_pools(self) -> Dict:
        """å»ºç«‹æ¨¡æ“¬è³‡æ–™æ± """
        return {
            'job_titles': [
                'Senior Data Engineer',
                'Data Engineer',
                'Staff Data Engineer', 
                'Principal Data Engineer',
                'Lead Data Engineer',
                'Data Engineer II',
                'Data Platform Engineer',
                'Senior Data Scientist',
                'Data Scientist',
                'Machine Learning Engineer',
                'Analytics Engineer',
                'Data Infrastructure Engineer',
                'Big Data Engineer',
                'Data Pipeline Engineer',
                'Senior Software Engineer - Data'
            ],
            
            'companies': [
                'Google', 'Meta', 'Amazon', 'Apple', 'Microsoft',
                'Netflix', 'Uber', 'Airbnb', 'Stripe', 'Shopify',
                'Snowflake', 'Databricks', 'Palantir', 'Coinbase',
                'Twitter', 'LinkedIn', 'Salesforce', 'Adobe',
                'Spotify', 'Slack', 'Zoom', 'DocuSign',
                'Square', 'PayPal', 'Tesla', 'SpaceX'
            ],
            
            'locations': [
                'San Francisco, CA',
                'Palo Alto, CA', 
                'Mountain View, CA',
                'Redwood City, CA',
                'San Jose, CA',
                'New York, NY',
                'Seattle, WA',
                'Austin, TX',
                'Los Angeles, CA',
                'Chicago, IL',
                'Boston, MA',
                'Denver, CO'
            ],
            
            'employment_types': [
                'Full-time',
                'Contract', 
                'Full-time (Permanent)',
                'Full-time - Permanent'
            ],
            
            'work_arrangements': [
                'Remote',
                'Hybrid',
                'On-site',
                'Remote (US)',
                'Hybrid (3 days in office)'
            ],
            
            'salary_ranges': [
                '$120,000 - $180,000',
                '$140,000 - $200,000', 
                '$160,000 - $220,000',
                '$180,000 - $250,000',
                '$200,000 - $280,000',
                '$100,000 - $150,000',
                '$130,000 - $170,000'
            ],
            
            'skills': [
                'Python', 'SQL', 'AWS', 'Spark', 'Kafka',
                'Docker', 'Kubernetes', 'Airflow', 'dbt',
                'Snowflake', 'Redshift', 'BigQuery', 'PostgreSQL',
                'MongoDB', 'Redis', 'Elasticsearch', 'Tableau',
                'Looker', 'Git', 'Jenkins', 'Terraform'
            ],
            
            'description_templates': [
                "We are looking for a {title} to join our growing data team. You will be responsible for building and maintaining data pipelines, working with large datasets, and collaborating with cross-functional teams.",
                
                "As a {title}, you will design and implement scalable data infrastructure, optimize data workflows, and ensure data quality across our platform.",
                
                "Join our data engineering team as a {title}! You'll work on cutting-edge data technologies, build real-time streaming pipelines, and help drive data-driven decisions.",
                
                "We're seeking a talented {title} to help us scale our data platform. You'll work with modern tools like Spark, Kafka, and cloud technologies.",
                
                "Exciting opportunity for a {title} to work on challenging data problems at scale. You'll be building the next generation of our data infrastructure."
            ]
        }
    
    def _generate_job_description(self, job_title: str, skills: List[str]) -> str:
        """ç”Ÿæˆæ¨¡æ“¬è·ä½æè¿°"""
        template = random.choice(self.mock_data['description_templates'])
        base_description = template.format(title=job_title)
        
        # åŠ å…¥æŠ€èƒ½è¦æ±‚
        skills_text = f"\n\nRequired Skills:\nâ€¢ {' â€¢ '.join(skills[:5])}"
        
        # åŠ å…¥é¡å¤–å…§å®¹
        additional_content = f"""
        
Responsibilities:
â€¢ Design and build scalable data pipelines
â€¢ Collaborate with data scientists and analysts
â€¢ Maintain and monitor data infrastructure
â€¢ Optimize data processing workflows
â€¢ Ensure data quality and reliability

Requirements:
â€¢ Bachelor's degree in Computer Science or related field
â€¢ 3+ years of experience in data engineering
â€¢ Strong programming skills in Python and SQL
â€¢ Experience with cloud platforms (AWS/GCP/Azure)
â€¢ Knowledge of distributed computing frameworks

Benefits:
â€¢ Competitive salary and equity
â€¢ Comprehensive health insurance
â€¢ Flexible work arrangements
â€¢ Learning and development budget
â€¢ Modern office with great snacks
        """
        
        return base_description + skills_text + additional_content
    
    def _generate_mock_job(self, index: int) -> Dict:
        """ç”Ÿæˆå–®ä¸€æ¨¡æ“¬è·ç¼º"""
        job_title = random.choice(self.mock_data['job_titles'])
        company = random.choice(self.mock_data['companies'])
        location = random.choice(self.mock_data['locations'])
        
        # éš¨æ©Ÿé¸æ“‡æŠ€èƒ½
        selected_skills = random.sample(self.mock_data['skills'], k=random.randint(3, 8))
        
        # ç”Ÿæˆå”¯ä¸€ job_id
        job_id = f"mock_job_{self.config['batch_id']}_{index:04d}"
        
        # ç”Ÿæˆæ¨¡æ“¬ URL
        job_url = f"https://www.linkedin.com/jobs/view/{random.randint(1000000000, 9999999999)}"
        
        # éš¨æ©Ÿæ±ºå®šæ˜¯å¦åŒ…å«è–ªè³‡è³‡è¨Š (70% æ©Ÿç‡)
        salary_range = random.choice(self.mock_data['salary_ranges']) if random.random() < 0.7 else ""
        
        # ç”Ÿæˆç™¼å¸ƒæ—¥æœŸ (éå» 1-7 å¤©)
        days_ago = random.randint(1, 7)
        posted_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')
        
        job_data = {
            'job_id': job_id,
            'job_url': job_url,
            'job_title': job_title,
            'company_name': company,
            'location': location,
            'employment_type': random.choice(self.mock_data['employment_types']),
            'work_arrangement': random.choice(self.mock_data['work_arrangements']),
            'salary_range': salary_range,
            'posted_date': posted_date,
            'job_description': self._generate_job_description(job_title, selected_skills),
            'scraped_at': datetime.now().isoformat(),
            'mock_data': True  # æ¨™è¨˜ç‚ºæ¨¡æ“¬è³‡æ–™
        }
        
        return job_data
    
    def scrape_jobs(self) -> List[Dict]:
        """æ¨¡æ“¬çˆ¬å–è·ç¼º - ä¸»è¦æ–¹æ³•"""
        target_jobs = self.config.get('target_jobs', 10)
        
        self.logger.info(f"ğŸ­ Starting MOCK LinkedIn job scraping for batch {self.config['batch_id']}")
        self.logger.info(f"ğŸ¯ Target: {target_jobs} mock jobs")
        
        # æ¨¡æ“¬çˆ¬å–éç¨‹
        for i in range(target_jobs):
            # æ¨¡æ“¬ç¶²è·¯å»¶é²
            delay = random.uniform(0.5, 2.0)  # æ¯”çœŸå¯¦çˆ¬å–å¿«ä¸€äº›
            time.sleep(delay)
            
            self.total_attempts += 1
            
            # æ¨¡æ“¬ 95% æˆåŠŸç‡
            if random.random() < 0.95:
                job_data = self._generate_mock_job(i)
                self.scraped_jobs.append(job_data)
                self.success_count += 1
                
                if (i + 1) % 5 == 0:
                    self.logger.info(f"ğŸ­ Mock progress: {i + 1}/{target_jobs} jobs generated")
            else:
                # æ¨¡æ“¬å¤±æ•—æƒ…æ³
                self.logger.warning(f"ğŸ­ Mock failure: simulated error for job {i + 1}")
        
        self.logger.info(f"ğŸ‰ Mock scraping completed: {len(self.scraped_jobs)} jobs generated")
        self.logger.info(f"ğŸ“Š Success rate: {self.get_success_rate():.1%}")
        
        return self.scraped_jobs
    
    def get_success_rate(self) -> float:
        """è¨ˆç®—æ¨¡æ“¬æˆåŠŸç‡"""
        if self.total_attempts == 0:
            return 0.0
        return self.success_count / self.total_attempts
    
    def get_scraping_stats(self) -> Dict:
        """å–å¾—æ¨¡æ“¬çˆ¬å–çµ±è¨ˆ"""
        return {
            'total_attempts': self.total_attempts,
            'successful_jobs': self.success_count,
            'failed_jobs': self.total_attempts - self.success_count,
            'success_rate': self.get_success_rate(),
            'scraped_jobs_count': len(self.scraped_jobs),
            'batch_id': self.config['batch_id'],
            'is_mock_data': True
        }


class MockLinkedInBasicScraper(MockLinkedInScraper):
    """
    ç‚ºäº†å…¼å®¹ç¾æœ‰ DAGï¼Œæä¾›èˆ‡çœŸå¯¦çˆ¬èŸ²ç›¸åŒçš„ä»‹é¢
    """
    pass