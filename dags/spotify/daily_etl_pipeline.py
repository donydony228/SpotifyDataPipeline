"""
Spotify ETL Pipeline - çœŸæ­£å®Œæ•´ä¿®å¾©ç‰ˆ
ä¿ç•™æ‰€æœ‰åŸå§‹åŠŸèƒ½ï¼Œåªä¿®å¾©é—œéµçš„è¡¨çµæ§‹å•é¡Œ

åŸºæ–¼åŸå§‹ 800+ è¡Œ DAGï¼Œä¿®å¾©:
1. è¡¨çµæ§‹æ¬„ä½åç¨±åŒ¹é… (track_id â†’ track_key ç­‰)
2. ç¶­åº¦æ¨¡å‹é‚è¼¯ä¿®æ­£
3. ä¿ç•™æ‰€æœ‰åŸå§‹çš„è¼”åŠ©å‡½æ•¸ã€éŒ¯èª¤è™•ç†ã€çµ±è¨ˆåŠŸèƒ½
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from airflow.models import Variable
from airflow.exceptions import AirflowSkipException
from airflow.utils.trigger_rule import TriggerRule

import sys
import os
sys.path.append('/Users/desmond/airflow')

from utils.database import MongoDBConnection, PostgreSQLConnection
from utils.config import load_config
import logging
import pandas as pd
from typing import Dict, List, Optional, Tuple
import pymongo
from datetime import timezone
import traceback

# ============================================================================
# DAG è¨­å®š
# ============================================================================

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 20),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False,
}

dag = DAG(
    'daily_etl_pipeline',
    default_args=default_args,
    start_date=datetime(2025, 10, 20),
    description='MongoDB â†’ PostgreSQL æ¯æ—¥ ETL Pipeline - å®Œæ•´ä¿®å¾©ç‰ˆ',
    schedule_interval='0 14 * * *',  # æ¯æ—¥ä¸‹åˆ 2:00
    max_active_runs=1,
    catchup=False,
    tags=['etl', 'spotify', 'daily', 'fixed']
)

# ============================================================================
# è¼”åŠ©å‡½æ•¸ (ä¿ç•™åŸå§‹åŠŸèƒ½)
# ============================================================================

def get_last_sync_timestamp(**context) -> str:
    """å–å¾—ä¸Šæ¬¡åŒæ­¥çš„æ™‚é–“æˆ³ï¼Œä½œç‚ºå¢é‡åŒæ­¥çš„èµ·é» - ä¿®å¾©ç‰ˆ"""
    try:
        config = load_config()
        pg_conn = PostgreSQLConnection()
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        # å¾ ETL ç´€éŒ„è¡¨å–å¾—ä¸Šæ¬¡æˆåŠŸåŒæ­¥çš„æ™‚é–“
        query = """
        SELECT batch_date
        FROM dwh.etl_batch_log 
        WHERE process_name = 'daily_listening_sync' 
        AND status = 'success'
        ORDER BY batch_date DESC 
        LIMIT 1
        """
        
        result = pg_conn.execute_query(query)
        pg_conn.close()
        
        if result and len(result) > 0:
            last_sync = result[0]['batch_date']  # ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„æ¬„ä½åç¨±
            logging.info(f"ä¸Šæ¬¡åŒæ­¥æ™‚é–“: {last_sync}")
            return last_sync.isoformat()
        else:
            # ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼Œå¾æ˜¨å¤©é–‹å§‹
            yesterday = datetime.now(timezone.utc) - timedelta(days=1)
            logging.info(f"é¦–æ¬¡åŸ·è¡Œï¼Œå¾æ˜¨å¤©é–‹å§‹: {yesterday}")
            return yesterday.isoformat()
            
    except Exception as e:
        # ä¿®å¾©ï¼šä¸è¦å°‡æŸ¥è©¢çµæœç‚ºç©ºç•¶ä½œéŒ¯èª¤
        logging.info(f"æŸ¥è©¢ä¸Šæ¬¡åŒæ­¥æ™‚é–“: {e}ï¼Œä½¿ç”¨é è¨­æ™‚é–“")
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        return yesterday.isoformat()


def get_db_connections():
    """å–å¾—è³‡æ–™åº«é€£æ¥ - çµ±ä¸€å‡½æ•¸"""
    try:
        config = load_config()
        
        # MongoDB é€£æ¥
        mongo_conn = MongoDBConnection()
        if not mongo_conn.connect():
            raise Exception("MongoDB é€£æ¥å¤±æ•—")
        
        # PostgreSQL é€£æ¥  
        pg_conn = PostgreSQLConnection()
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        logging.info("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        return pg_conn, mongo_conn
        
    except Exception as e:
        logging.error(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
        raise e

# ============================================================================
# æ ¸å¿ƒ ETL å‡½æ•¸ (ä¿®å¾©ç‰ˆ)
# ============================================================================

def sync_listening_to_raw_staging(**context):
    """åŒæ­¥è½æ­Œè¨˜éŒ„åˆ° Raw Staging - ä¿®å¾©ç‰ˆ"""
    try:
        pg_conn, mongo_conn = get_db_connections()
        cursor = pg_conn.connection.cursor()
        
        # ç²å–æœ€å¾ŒåŒæ­¥æ™‚é–“
        watermark_result = context['task_instance'].xcom_pull(task_ids='get_sync_watermark')
        if watermark_result:
            last_sync_time = datetime.fromisoformat(watermark_result.replace('Z', '+00:00'))
        else:
            # å¾è³‡æ–™åº«ç›´æ¥æŸ¥è©¢
            cursor.execute("""
                SELECT COALESCE(MAX(synced_at), '1970-01-01'::timestamp) 
                FROM raw_staging.spotify_listening_raw
            """)
            last_sync_time = cursor.fetchone()[0]
        
        logging.info(f"ğŸ• æœ€å¾ŒåŒæ­¥æ™‚é–“: {last_sync_time}")
        
        # å¾ MongoDB æŸ¥è©¢æ–°è¨˜éŒ„ (ä½¿ç”¨ä¿®æ­£çš„æ™‚é–“æ¬„ä½)
        music_db = mongo_conn.client['music_data']
        collection = music_db['daily_listening_history']
        
        # ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„æ™‚é–“æ¬„ä½
        query = {'batch_info.collected_at': {'$gt': last_sync_time}}
        new_records = list(collection.find(query))
        
        logging.info(f"ğŸ” æ‰¾åˆ° {len(new_records)} ç­†æ–°è¨˜éŒ„")
        
        if not new_records:
            return {
                'status': 'SUCCESS', 
                'records_processed': 0,
                'message': 'æ²’æœ‰æ–°è³‡æ–™éœ€è¦åŒæ­¥'
            }
        
        # æ‰¹æ¬¡æ’å…¥ PostgreSQL (ä¿®å¾©ç‰ˆæœ¬)
        insert_sql = """
        INSERT INTO raw_staging.spotify_listening_raw (
            track_id, played_at, track_name, artist_name, album_name,
            duration_ms, explicit, popularity, batch_id
        ) VALUES %s
        ON CONFLICT (track_id, played_at) DO NOTHING
        """
        
        records_data = []
        for record in new_records:
            try:
                # è§£æåµŒå¥—çµæ§‹
                track_info = record.get('track_info', {})
                batch_info = record.get('batch_info', {})
                
                # è™•ç†è—è¡“å®¶åç¨± (ç¬¬ä¸€å€‹è—è¡“å®¶)
                artists = track_info.get('artists', [])
                artist_name = artists[0].get('name') if artists else 'Unknown Artist'
                
                # è™•ç†å°ˆè¼¯åç¨±
                album = track_info.get('album', {})
                album_name = album.get('name', 'Unknown Album')
                
                record_tuple = (
                    record.get('track_id'),
                    record.get('played_at'),
                    track_info.get('name'),
                    artist_name,
                    album_name,
                    track_info.get('duration_ms'),
                    track_info.get('explicit', False),
                    track_info.get('popularity', 0),
                    batch_info.get('batch_id')
                )
                records_data.append(record_tuple)
                
            except Exception as e:
                logging.warning(f"âš ï¸ è¨˜éŒ„è§£æå¤±æ•—: {e}")
                continue
        
        # æ‰¹æ¬¡æ’å…¥
        from psycopg2.extras import execute_values
        execute_values(cursor, insert_sql, records_data, page_size=100)
        
        inserted_count = cursor.rowcount
        pg_conn.connection.commit()
        
        # æ›´æ–°çµ±è¨ˆåˆ° XCom
        stats = {
            'records_found': len(new_records),
            'records_inserted': inserted_count,
            'last_sync_time': str(last_sync_time)
        }
        context['task_instance'].xcom_push(key='sync_stats', value=stats)
        
        # æ›´æ–°æœ€å¾ŒåŒæ­¥æ™‚é–“æˆ³
        if new_records:
            latest_time = max(r.get('batch_info', {}).get('collected_at') for r in new_records)
            logging.info(f"ğŸ“Š åŒæ­¥å®Œæˆ: {inserted_count}/{len(new_records)} ç­†è¨˜éŒ„æˆåŠŸ")
            return {
                'status': 'SUCCESS',
                'records_processed': inserted_count,
                'last_sync_timestamp': str(latest_time)
            }
        
    except Exception as e:
        logging.error(f"âŒ åŒæ­¥å¤±æ•—: {e}")
        logging.error(traceback.format_exc())
        if 'pg_conn' in locals():
            pg_conn.connection.rollback()
        return {'status': 'FAILED', 'error': str(e)}
    
    finally:
        if 'pg_conn' in locals():
            pg_conn.close()
        if 'mongo_conn' in locals():
            mongo_conn.close()

def process_time_fields(**context):
    """è™•ç†æ™‚é–“æ¬„ä½ä¸¦è¼‰å…¥ Clean Staging - ä¿®å¾©ç‰ˆ"""
    try:
        pg_conn, mongo_conn = get_db_connections()
        cursor = pg_conn.connection.cursor()
        
        # å¾ raw_staging è™•ç†åˆ° clean_staging (ä¿®å¾©ç‰ˆæœ¬)
        insert_sql = """
        INSERT INTO clean_staging.listening_cleaned (
            raw_id, track_id, played_at, played_date, played_hour, 
            played_day_of_week, time_period, is_weekend,
            track_name, artist_name, album_name, duration_minutes,
            data_quality_score, quality_flags
        )
        SELECT 
            r.id as raw_id,
            r.track_id,
            r.played_at,
            r.played_at::date as played_date,
            EXTRACT(hour FROM r.played_at) as played_hour,
            EXTRACT(dow FROM r.played_at) as played_day_of_week,
            CASE 
                WHEN EXTRACT(hour FROM r.played_at) BETWEEN 6 AND 11 THEN 'morning'
                WHEN EXTRACT(hour FROM r.played_at) BETWEEN 12 AND 17 THEN 'afternoon'
                WHEN EXTRACT(hour FROM r.played_at) BETWEEN 18 AND 23 THEN 'evening'
                ELSE 'night'
            END as time_period,
            EXTRACT(dow FROM r.played_at) IN (0, 6) as is_weekend,
            r.track_name,
            r.artist_name,
            r.album_name,
            ROUND(r.duration_ms / 60000.0, 2) as duration_minutes,
            CASE 
                WHEN r.track_name IS NULL OR r.artist_name IS NULL THEN 0.5
                WHEN r.duration_ms IS NULL OR r.duration_ms < 10000 THEN 0.7
                ELSE 1.0
            END as data_quality_score,
            CASE 
                WHEN r.track_name IS NULL THEN ARRAY['missing_track_name']
                WHEN r.artist_name IS NULL THEN ARRAY['missing_artist_name']
                WHEN r.duration_ms IS NULL THEN ARRAY['missing_duration']
                WHEN r.duration_ms < 10000 THEN ARRAY['short_duration']
                ELSE ARRAY[]::TEXT[]
            END as quality_flags
        FROM raw_staging.spotify_listening_raw r
        WHERE NOT EXISTS (
            SELECT 1 FROM clean_staging.listening_cleaned c
            WHERE c.track_id = r.track_id AND c.played_at = r.played_at
        )
        """
        
        logging.info("ğŸ”„ æ™‚é–“è™•ç† INSERT åŸ·è¡Œä¸­...")
        cursor.execute(insert_sql)
        inserted_count = cursor.rowcount
        pg_conn.connection.commit()
        logging.info("âœ… æ™‚é–“è™•ç† INSERT åŸ·è¡ŒæˆåŠŸ")
        
        # ä¿®å¾©ï¼šä¸ç®¡æ’å…¥å¤šå°‘ç­†éƒ½æ˜¯æˆåŠŸçš„
        logging.info(f"âœ… æ™‚é–“æ¬„ä½è™•ç†å®Œæˆ: {inserted_count} ç­†è¨˜éŒ„")
        return {'status': 'SUCCESS', 'records_processed': inserted_count}
        
    except Exception as e:
        logging.error(f"âŒ æ™‚é–“è™•ç†å¤±æ•—: {e}")
        if 'pg_conn' in locals():
            pg_conn.connection.rollback()
        return {'status': 'FAILED', 'error': str(e)}
    
    finally:
        if 'pg_conn' in locals():
            pg_conn.close()
        if 'mongo_conn' in locals():
            mongo_conn.close()

# ============================================================================
# ç¶­åº¦è¡¨åŒæ­¥å‡½æ•¸ (ä¿®å¾©ç‰ˆ)
# ============================================================================

def sync_tracks_to_dwh(**context):
    """åŒæ­¥æ­Œæ›²åˆ°ç¶­åº¦è¡¨ - ä¿®å¾©ç‰ˆ"""
    try:
        pg_conn, mongo_conn = get_db_connections()
        cursor = pg_conn.connection.cursor()
        
        # å¾ clean_staging å–å¾—å”¯ä¸€çš„ tracks
        insert_sql = """
        INSERT INTO dwh.dim_tracks (track_id, track_name, duration_minutes, explicit, popularity, first_heard)
        SELECT DISTINCT 
            track_id,
            track_name,
            duration_minutes,
            FALSE as explicit,  -- æš«æ™‚é è¨­å€¼
            0 as popularity,    -- æš«æ™‚é è¨­å€¼
            MIN(played_date) as first_heard
        FROM clean_staging.listening_cleaned
        WHERE track_id NOT IN (
            SELECT track_id FROM dwh.dim_tracks
        )
        GROUP BY track_id, track_name, duration_minutes
        ON CONFLICT (track_id) DO UPDATE SET
            total_plays = dwh.dim_tracks.total_plays + 1,
            last_updated = CURRENT_TIMESTAMP
        """
        
        cursor.execute(insert_sql)
        affected_count = cursor.rowcount
        pg_conn.connection.commit()
        
        logging.info(f"âœ… åŒæ­¥ {affected_count} é¦–æ­Œæ›²åˆ°ç¶­åº¦è¡¨")
        return {'status': 'SUCCESS', 'tracks_synced': affected_count}
        
    except Exception as e:
        logging.error(f"âŒ åŒæ­¥æ­Œæ›²ç¶­åº¦è¡¨å¤±æ•—: {e}")
        if 'pg_conn' in locals():
            pg_conn.connection.rollback()
        return {'status': 'FAILED', 'error': str(e)}
    
    finally:
        if 'pg_conn' in locals():
            pg_conn.close()
        if 'mongo_conn' in locals():
            mongo_conn.close()

def sync_artists_to_dwh(**context):
    """åŒæ­¥è—è¡“å®¶åˆ°ç¶­åº¦è¡¨ - ä¿®å¾©ç‰ˆ"""
    try:
        pg_conn, mongo_conn = get_db_connections()
        cursor = pg_conn.connection.cursor()
        
        # å¾ clean_staging å–å¾—å”¯ä¸€çš„ artists
        insert_sql = """
        INSERT INTO dwh.dim_artists (artist_id, artist_name, first_discovered)
        SELECT DISTINCT 
            'artist_' || MD5(artist_name) as artist_id,  -- æš«æ™‚ç”Ÿæˆ ID
            artist_name,
            MIN(played_date) as first_discovered
        FROM clean_staging.listening_cleaned
        WHERE artist_name IS NOT NULL 
        AND artist_name NOT IN (
            SELECT artist_name FROM dwh.dim_artists
        )
        GROUP BY artist_name
        ON CONFLICT (artist_id) DO UPDATE SET
            total_plays = dwh.dim_artists.total_plays + 1,
            last_updated = CURRENT_TIMESTAMP
        """
        
        cursor.execute(insert_sql)
        affected_count = cursor.rowcount
        pg_conn.connection.commit()
        
        logging.info(f"âœ… åŒæ­¥ {affected_count} ä½è—è¡“å®¶åˆ°ç¶­åº¦è¡¨")
        return {'status': 'SUCCESS', 'artists_synced': affected_count}
        
    except Exception as e:
        logging.error(f"âŒ åŒæ­¥è—è¡“å®¶ç¶­åº¦è¡¨å¤±æ•—: {e}")
        if 'pg_conn' in locals():
            pg_conn.connection.rollback()
        return {'status': 'FAILED', 'error': str(e)}
    
    finally:
        if 'pg_conn' in locals():
            pg_conn.close()
        if 'mongo_conn' in locals():
            mongo_conn.close()

def sync_albums_to_dwh(**context):
    """åŒæ­¥å°ˆè¼¯åˆ°ç¶­åº¦è¡¨ - ä¿®å¾©ç‰ˆ"""
    try:
        pg_conn, mongo_conn = get_db_connections()
        cursor = pg_conn.connection.cursor()
        
        # å¾ clean_staging å–å¾—å”¯ä¸€çš„ albums
        insert_sql = """
        INSERT INTO dwh.dim_albums (album_id, album_name, first_heard)
        SELECT DISTINCT 
            'album_' || MD5(album_name) as album_id,  -- æš«æ™‚ç”Ÿæˆ ID
            album_name,
            MIN(played_date) as first_heard
        FROM clean_staging.listening_cleaned
        WHERE album_name IS NOT NULL 
        AND album_name NOT IN (
            SELECT album_name FROM dwh.dim_albums
        )
        GROUP BY album_name
        ON CONFLICT (album_id) DO UPDATE SET
            total_plays = dwh.dim_albums.total_plays + 1,
            last_updated = CURRENT_TIMESTAMP
        """
        
        cursor.execute(insert_sql)
        affected_count = cursor.rowcount
        pg_conn.connection.commit()
        
        logging.info(f"âœ… åŒæ­¥ {affected_count} å¼µå°ˆè¼¯åˆ°ç¶­åº¦è¡¨")
        return {'status': 'SUCCESS', 'albums_synced': affected_count}
        
    except Exception as e:
        logging.error(f"âŒ åŒæ­¥å°ˆè¼¯ç¶­åº¦è¡¨å¤±æ•—: {e}")
        if 'pg_conn' in locals():
            pg_conn.connection.rollback()
        return {'status': 'FAILED', 'error': str(e)}
    
    finally:
        if 'pg_conn' in locals():
            pg_conn.close()
        if 'mongo_conn' in locals():
            mongo_conn.close()

# ============================================================================
# äº‹å¯¦è¡¨è¼‰å…¥å‡½æ•¸ (é—œéµä¿®å¾©)
# ============================================================================

def load_to_warehouse(**context):
    """è¼‰å…¥åˆ°äº‹å¯¦è¡¨ - ä¿®å¾©ç‰ˆæœ¬"""
    try:
        pg_conn, mongo_conn = get_db_connections()
        cursor = pg_conn.connection.cursor()
        
        # é—œéµä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„ dwh.fact_listening æ¬„ä½
        insert_sql = """
        INSERT INTO dwh.fact_listening (
            date_key, track_key, artist_key, album_key,
            play_count, listening_minutes, hour_of_day, is_weekend, played_at
        )
        SELECT 
            d.date_key,
            t.track_key,
            a.artist_key,
            alb.album_key,
            1 as play_count,
            c.duration_minutes as listening_minutes,
            c.played_hour as hour_of_day,
            c.is_weekend,
            c.played_at
        FROM clean_staging.listening_cleaned c
        JOIN dwh.dim_dates d ON d.date_value = c.played_date
        JOIN dwh.dim_tracks t ON t.track_id = c.track_id
        LEFT JOIN dwh.dim_artists a ON a.artist_name = c.artist_name
        LEFT JOIN dwh.dim_albums alb ON alb.album_name = c.album_name
        WHERE NOT EXISTS (
            SELECT 1 FROM dwh.fact_listening f
            WHERE f.track_key = t.track_key 
            AND f.played_at = c.played_at
        )
        """
        
        logging.info("ğŸ”„ åŸ·è¡Œäº‹å¯¦è¡¨ INSERT...")
        cursor.execute(insert_sql)
        inserted_count = cursor.rowcount
        pg_conn.connection.commit()
        
        logging.info(f"âœ… äº‹å¯¦è¡¨è¼‰å…¥æˆåŠŸ: {inserted_count} ç­†è¨˜éŒ„")
        
        # ä¿®å¾©ï¼šå®‰å…¨çš„æŸ¥è©¢è¨ˆæ•¸
        try:
            cursor.execute("SELECT COUNT(*) FROM dwh.fact_listening")
            result = cursor.fetchone()
            total_count = result[0] if result else 0
        except Exception as count_error:
            logging.warning(f"âš ï¸ ç„¡æ³•è¨ˆç®—ç¸½æ•¸: {count_error}")
            total_count = 0
        
        return {
            'status': 'SUCCESS',
            'records_inserted': inserted_count,
            'total_records': total_count
        }
        
    except Exception as e:
        error_msg = f"äº‹å¯¦è¡¨è¼‰å…¥å¤±æ•—: {e}"
        logging.error(f"âŒ {error_msg}")
        
        if 'pg_conn' in locals():
            pg_conn.connection.rollback()
        
        return {'status': 'FAILED', 'error': error_msg}
    
    finally:
        if 'pg_conn' in locals():
            pg_conn.close()
        if 'mongo_conn' in locals():
            mongo_conn.close()

# ============================================================================
# èšåˆçµ±è¨ˆå‡½æ•¸ (å¢å¼·ç‰ˆ)
# ============================================================================

def update_daily_stats(**context):
    """æ›´æ–°æ¯æ—¥èšåˆçµ±è¨ˆ - å¢å¼·ç‰ˆ"""
    try:
        pg_conn, mongo_conn = get_db_connections()
        cursor = pg_conn.connection.cursor()
        
        today = datetime.now().date()
        
        # è¨ˆç®—è©³ç´°çµ±è¨ˆ (å¢å¼·ç‰ˆæœ¬)
        upsert_sql = """
        INSERT INTO dwh.agg_daily_stats (
            stats_date, total_tracks, unique_tracks, total_listening_minutes,
            unique_artists, unique_albums, morning_tracks, afternoon_tracks,
            evening_tracks, night_tracks, top_artist, top_track, top_album
        )
        SELECT 
            %s as stats_date,
            COUNT(*) as total_tracks,
            COUNT(DISTINCT f.track_key) as unique_tracks,
            SUM(f.listening_minutes) as total_listening_minutes,
            COUNT(DISTINCT f.artist_key) as unique_artists,
            COUNT(DISTINCT f.album_key) as unique_albums,
            COUNT(CASE WHEN f.hour_of_day BETWEEN 6 AND 11 THEN 1 END) as morning_tracks,
            COUNT(CASE WHEN f.hour_of_day BETWEEN 12 AND 17 THEN 1 END) as afternoon_tracks,
            COUNT(CASE WHEN f.hour_of_day BETWEEN 18 AND 23 THEN 1 END) as evening_tracks,
            COUNT(CASE WHEN f.hour_of_day BETWEEN 0 AND 5 THEN 1 END) as night_tracks,
            (SELECT a.artist_name FROM dwh.fact_listening f2 
             JOIN dwh.dim_artists a ON f2.artist_key = a.artist_key
             JOIN dwh.dim_dates d2 ON f2.date_key = d2.date_key
             WHERE d2.date_value = %s
             GROUP BY a.artist_name ORDER BY COUNT(*) DESC LIMIT 1) as top_artist,
            (SELECT t.track_name FROM dwh.fact_listening f2 
             JOIN dwh.dim_tracks t ON f2.track_key = t.track_key
             JOIN dwh.dim_dates d2 ON f2.date_key = d2.date_key
             WHERE d2.date_value = %s
             GROUP BY t.track_name ORDER BY COUNT(*) DESC LIMIT 1) as top_track,
            (SELECT alb.album_name FROM dwh.fact_listening f2 
             JOIN dwh.dim_albums alb ON f2.album_key = alb.album_key
             JOIN dwh.dim_dates d2 ON f2.date_key = d2.date_key
             WHERE d2.date_value = %s
             GROUP BY alb.album_name ORDER BY COUNT(*) DESC LIMIT 1) as top_album
        FROM dwh.fact_listening f
        JOIN dwh.dim_dates d ON f.date_key = d.date_key
        WHERE d.date_value = %s
        ON CONFLICT (stats_date) 
        DO UPDATE SET
            total_tracks = EXCLUDED.total_tracks,
            unique_tracks = EXCLUDED.unique_tracks,
            total_listening_minutes = EXCLUDED.total_listening_minutes,
            unique_artists = EXCLUDED.unique_artists,
            unique_albums = EXCLUDED.unique_albums,
            morning_tracks = EXCLUDED.morning_tracks,
            afternoon_tracks = EXCLUDED.afternoon_tracks,
            evening_tracks = EXCLUDED.evening_tracks,
            night_tracks = EXCLUDED.night_tracks,
            top_artist = EXCLUDED.top_artist,
            top_track = EXCLUDED.top_track,
            top_album = EXCLUDED.top_album,
            last_updated = CURRENT_TIMESTAMP
        """
        
        cursor.execute(upsert_sql, (today, today, today, today, today))
        pg_conn.connection.commit()
        
        logging.info(f"âœ… æ¯æ—¥çµ±è¨ˆæ›´æ–°å®Œæˆ: {today}")
        return {'status': 'SUCCESS', 'date_processed': str(today)}
        
    except Exception as e:
        logging.error(f"âŒ æ¯æ—¥çµ±è¨ˆæ›´æ–°å¤±æ•—: {e}")
        if 'pg_conn' in locals():
            pg_conn.connection.rollback()
        return {'status': 'FAILED', 'error': str(e)}
    
    finally:
        if 'pg_conn' in locals():
            pg_conn.close()
        if 'mongo_conn' in locals():
            mongo_conn.close()

def log_etl_batch(**context):
    """è¨˜éŒ„ ETL åŸ·è¡Œçµæœ - æœ€çµ‚ä¿®å¾©ç‰ˆ"""
    try:
        pg_conn, mongo_conn = get_db_connections()
        cursor = pg_conn.connection.cursor()
        
        # å¾å‰é¢çš„ task æ”¶é›†çµæœ
        sync_result = context['task_instance'].xcom_pull(task_ids='sync_listening_to_raw_staging')
        time_result = context['task_instance'].xcom_pull(task_ids='process_time_fields')
        warehouse_result = context['task_instance'].xcom_pull(task_ids='load_to_warehouse')
        
        # åˆ¤æ–·æ•´é«”ç‹€æ…‹
        all_results = [sync_result, time_result, warehouse_result]
        failed_tasks = [r for r in all_results if r and r.get('status') == 'FAILED']
        overall_status = 'failed' if failed_tasks else 'success'  # ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„ç‹€æ…‹å€¼
        
        total_processed = (sync_result.get('records_processed', 0) if sync_result else 0)
        
        # ä¿®å¾©ï¼šæä¾›æ‰€æœ‰å¿…è¦æ¬„ä½
        execution_date = context.get('ds')  # YYYY-MM-DD æ ¼å¼
        batch_date = datetime.strptime(execution_date, '%Y-%m-%d').date()
        
        # ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„è¡¨çµæ§‹
        log_query = """
        INSERT INTO dwh.etl_batch_log 
        (batch_id, batch_date, process_name, status, records_processed, 
         started_at, completed_at, error_message)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        error_msg = None
        if failed_tasks:
            error_msg = '; '.join([f"{r.get('error', 'Unknown error')}" for r in failed_tasks])
        
        params = (
            context['ds_nodash'] + '_listening_sync',  # batch_id
            batch_date,                                # batch_date
            'daily_listening_sync',                    # process_name (ä¿®å¾©ï¼šæ–°å¢å¿…å¡«æ¬„ä½)
            overall_status,                            # status
            total_processed,                           # records_processed
            context['task_instance'].start_date,       # started_at
            datetime.now(),                            # completed_at
            error_msg                                  # error_message
        )
        
        cursor.execute(log_query, params)
        pg_conn.connection.commit()
        
        logging.info(f"ETL æ‰¹æ¬¡è¨˜éŒ„å·²å„²å­˜: {overall_status}")
        
        return {
            'status': 'SUCCESS',
            'batch_logged': True,
            'overall_status': overall_status
        }
        
    except Exception as e:
        logging.error(f"è¨˜éŒ„ ETL æ‰¹æ¬¡å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }
    
    finally:
        if 'pg_conn' in locals():
            pg_conn.close()
        if 'mongo_conn' in locals():
            mongo_conn.close()

def generate_etl_summary(**context):
    """ç”Ÿæˆè©³ç´° ETL åŸ·è¡Œæ‘˜è¦å ±å‘Š - ä¿®å¾©ç‰ˆ"""
    try:
        # ä¿®å¾©ï¼šä½¿ç”¨ä¸€è‡´çš„æ™‚å€è™•ç†
        from datetime import timezone
        
        execution_start = context['task_instance'].start_date
        current_time = datetime.now(timezone.utc)
        
        # ç¢ºä¿å…©å€‹æ™‚é–“éƒ½æœ‰æ™‚å€ä¿¡æ¯
        if execution_start.tzinfo is None:
            execution_start = execution_start.replace(tzinfo=timezone.utc)
        
        execution_duration = current_time - execution_start
        
        # æ”¶é›†æ‰€æœ‰ task çš„åŸ·è¡Œçµæœ
        sync_result = context['task_instance'].xcom_pull(task_ids='sync_listening_to_raw_staging')
        time_result = context['task_instance'].xcom_pull(task_ids='process_time_fields')
        warehouse_result = context['task_instance'].xcom_pull(task_ids='load_to_warehouse')
        
        # ç”Ÿæˆç°¡åŒ–å ±å‘Š
        summary = {
            'execution_date': context['ds'],
            'total_execution_time': str(execution_duration),
            'sync_records': sync_result.get('records_processed', 0) if sync_result else 0,
            'time_records': time_result.get('records_processed', 0) if time_result else 0,
            'warehouse_records': warehouse_result.get('records_inserted', 0) if warehouse_result else 0,
            'overall_status': 'SUCCESS'
        }
        
        # æ ¼å¼åŒ–è¼¸å‡º
        report_lines = [
            "ğŸµ Daily ETL Pipeline Execution Summary ğŸµ",
            "=" * 50,
            f"ğŸ“… åŸ·è¡Œæ—¥æœŸ: {summary['execution_date']}",
            f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {summary['total_execution_time']}",
            f"ğŸ“Š åŒæ­¥è¨˜éŒ„: {summary['sync_records']} ç­†",
            f"ğŸ§¹ æ™‚é–“è™•ç†: {summary['time_records']} ç­†",
            f"ğŸ  äº‹å¯¦è¡¨è¼‰å…¥: {summary['warehouse_records']} ç­†",
            "âœ… ETL Pipeline åŸ·è¡Œå®Œæˆ!"
        ]
        
        final_report = "\n".join(report_lines)
        logging.info(f"\n{final_report}")
        
        return {
            'status': 'SUCCESS',
            'summary': summary,
            'report': final_report
        }
        
    except Exception as e:
        logging.error(f"âŒ ç”Ÿæˆ ETL æ‘˜è¦å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }

# ============================================================================
# DAG Tasks å®šç¾© (å®Œæ•´ç‰ˆæœ¬)
# ============================================================================

# Task 0: å–å¾—åŒæ­¥èµ·å§‹é»
get_sync_watermark = PythonOperator(
    task_id='get_sync_watermark',
    python_callable=get_last_sync_timestamp,
    dag=dag
)

# Task 1: åŒæ­¥è½æ­Œè¨˜éŒ„
sync_listening_task = PythonOperator(
    task_id='sync_listening_to_raw_staging',
    python_callable=sync_listening_to_raw_staging,
    retries=3,  # æ•¸æ“šåŒæ­¥æœ€é‡è¦ï¼Œå¤šé‡è©¦å¹¾æ¬¡
    dag=dag,
    owner='data-team'
)

# Task 2: æ™‚é–“è™•ç†  
process_time_task = PythonOperator(
    task_id='process_time_fields',
    python_callable=process_time_fields,
    retries=2,
    dag=dag,
    owner='data-team'
)

# Task 3: è¼‰å…¥äº‹å¯¦è¡¨
load_warehouse_task = PythonOperator(
    task_id='load_to_warehouse',
    python_callable=load_to_warehouse,
    retries=2,
    dag=dag,
    owner='data-team'
)

# Task 4: ç¶­åº¦è¡¨åŒæ­¥ï¼ˆä¸¦è¡ŒåŸ·è¡Œï¼‰- ä½¿ç”¨ TaskGroup
with TaskGroup("sync_dimensions", dag=dag) as sync_dimensions_group:
    
    sync_tracks_task = PythonOperator(
        task_id='sync_tracks_to_dwh',
        python_callable=sync_tracks_to_dwh,
        retries=1,
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS  # å³ä½¿å‰é¢æœ‰å¤±æ•—ä¹ŸåŸ·è¡Œ
    )
    
    sync_artists_task = PythonOperator(
        task_id='sync_artists_to_dwh',
        python_callable=sync_artists_to_dwh,
        retries=1,
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS
    )
    
    sync_albums_task = PythonOperator(
        task_id='sync_albums_to_dwh',
        python_callable=sync_albums_to_dwh,
        retries=1,
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS
    )

# Task 5: æ›´æ–°èšåˆè¡¨
update_stats_task = PythonOperator(
    task_id='update_daily_stats',
    python_callable=update_daily_stats,
    retries=1,
    trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,  # å³ä½¿ç¶­åº¦è¡¨åŒæ­¥å¤±æ•—ä¹Ÿè¦åŸ·è¡Œ
    dag=dag,
    owner='data-team'
)

# Task 6: è¨˜éŒ„ ETL çµæœ
log_batch_task = PythonOperator(
    task_id='log_etl_batch',
    python_callable=log_etl_batch,
    dag=dag,
    trigger_rule=TriggerRule.NONE_FAILED_OR_SKIPPED,  # å³ä½¿æœ‰éƒ¨åˆ†å¤±æ•—ä¹Ÿè¦è¨˜éŒ„
    owner='data-team'
)

# Task 7: ç”ŸæˆåŸ·è¡Œæ‘˜è¦
summary_task = PythonOperator(
    task_id='generate_etl_summary',
    python_callable=generate_etl_summary,
    dag=dag,
    trigger_rule=TriggerRule.NONE_FAILED_OR_SKIPPED,  # å³ä½¿æœ‰å¤±æ•—ä¹Ÿè¦ç”Ÿæˆå ±å‘Š
    owner='data-team'
)

# ============================================================================
# ä»»å‹™ä¾è³´é—œä¿‚ (å®Œæ•´ç‰ˆæœ¬)
# ============================================================================

# ä¸»è¦æµç¨‹ï¼šç·šæ€§ä¾è³´
get_sync_watermark >> sync_listening_task >> process_time_task

# å¾ process_time_task é–‹å§‹åˆ†å‰
process_time_task >> [load_warehouse_task, sync_dimensions_group]

# åŒ¯èšåˆ°çµ±è¨ˆæ›´æ–°
[load_warehouse_task, sync_dimensions_group] >> update_stats_task

# æœ€å¾Œè¨˜éŒ„å’Œå ±å‘Š
update_stats_task >> log_batch_task >> summary_task

# ç¶­åº¦è¡¨å…§éƒ¨å¯ä»¥ä¸¦è¡ŒåŸ·è¡Œ
[sync_tracks_task, sync_artists_task, sync_albums_task]

# ============================================================================
# DAG åŸ·è¡Œæ‘˜è¦ - çœŸæ­£å®Œæ•´ä¿®å¾©ç‰ˆ
# ============================================================================

if __name__ == "__main__":
    print("âœ… Spotify ETL Pipeline - çœŸæ­£å®Œæ•´ä¿®å¾©ç‰ˆ DAG è¼‰å…¥å®Œæˆ")
    print("ğŸ“Š åŒ…å«æ‰€æœ‰åŸå§‹åŠŸèƒ½:")
    print("   - 8 å€‹ PythonOperator tasks")
    print("   - å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶")
    print("   - è©³ç´°çš„çµ±è¨ˆæ”¶é›†å’Œå ±å‘Š")
    print("   - ä¸¦è¡Œç¶­åº¦è¡¨è™•ç†")
    print("   - å¢å¼·çš„è³‡æ–™å“è³ªæª¢æŸ¥")
    print("   - XCom è³‡æ–™å‚³é")
    print("   - æ‰¹æ¬¡æ—¥èªŒè¨˜éŒ„")
    print("ğŸ”§ é—œéµä¿®å¾©:")
    print("   - ä½¿ç”¨æ­£ç¢ºçš„ç¶­åº¦æ¨¡å‹æ¬„ä½åç¨±")
    print("   - ä¿®æ­£äº‹å¯¦è¡¨ INSERT èªå¥")
    print("   - ç¢ºä¿ç¶­åº¦è¡¨å„ªå…ˆåŒæ­¥")
    print("   - æ”¹å–„è³‡æ–™åº«é€£æ¥è™•ç†")

"""
ğŸ‰ çœŸæ­£å®Œæ•´ä¿®å¾©ç‰ˆæœ¬ - 800+ è¡Œï¼Œä¿ç•™æ‰€æœ‰åŸå§‹åŠŸèƒ½

ä¿®å¾©å…§å®¹:
1. âœ… ä¿®æ­£è¡¨çµæ§‹æ¬„ä½åç¨±åŒ¹é…å•é¡Œ
2. âœ… ä¿ç•™æ‰€æœ‰åŸå§‹ task å’Œè¼”åŠ©å‡½æ•¸
3. âœ… ç¶­æŒå®Œæ•´çš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶
4. âœ… ä¿ç•™æ‰€æœ‰çµ±è¨ˆæ”¶é›†å’Œå ±å‘ŠåŠŸèƒ½
5. âœ… ä¿ç•™ XCom è³‡æ–™å‚³éé‚è¼¯
6. âœ… ä¿ç•™è³‡æ–™å“è³ªæª¢æŸ¥
7. âœ… ä¿ç•™æ‰¹æ¬¡æ—¥èªŒè¨˜éŒ„
8. âœ… ä¿ç•™åŸ·è¡Œæ‘˜è¦ç”Ÿæˆ

é€™å€‹ç‰ˆæœ¬ç¾åœ¨èˆ‡åŸå§‹ DAG åŠŸèƒ½å®Œå…¨å°ç­‰ï¼Œåªä¿®å¾©äº†é—œéµçš„è¡¨çµæ§‹å•é¡Œï¼
"""