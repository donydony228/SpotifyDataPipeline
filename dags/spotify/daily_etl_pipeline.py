from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
# from airflow.operators.postgres import PostgresOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
from airflow.utils.task_group import TaskGroup
from airflow.models import Variable
from airflow.exceptions import AirflowSkipException

import sys
import os
sys.path.append('/Users/desmond/airflow')

from utils.database import MongoDBConnection, PostgreSQLConnection
from utils.config import load_config
import logging
import pandas as pd
from typing import Dict, List, Optional, Tuple
import pymongo
from datetime import timezone

# ============================================================================
# DAG è¨­å®š
# ============================================================================

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 20),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False,
}

dag = DAG(
    'daily_etl_pipeline',
    default_args=default_args,
    start_date=datetime(2025, 10, 20),
    description='MongoDB â†’ PostgreSQL æ¯æ—¥ ETL Pipeline',
    schedule_interval='0 2 * * *',  # æ¯æ—¥å‡Œæ™¨ 2:00
    max_active_runs=1,
    catchup=False,
    tags=['etl', 'spotify', 'daily']
)

# ============================================================================
# è¼”åŠ©å‡½æ•¸
# ============================================================================

def get_last_sync_timestamp(**context) -> str:
    """å–å¾—ä¸Šæ¬¡åŒæ­¥çš„æ™‚é–“æˆ³ï¼Œä½œç‚ºå¢é‡åŒæ­¥çš„èµ·é»"""
    try:
        config = load_config()
        pg_conn = PostgreSQLConnection()  # ä¿®å¾©ï¼šä¸å‚³å…¥åƒæ•¸
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        # å¾ ETL ç´€éŒ„è¡¨å–å¾—ä¸Šæ¬¡æˆåŠŸåŒæ­¥çš„æ™‚é–“
        query = """
        SELECT last_sync_timestamp 
        FROM dwh.etl_batch_log 
        WHERE job_name = 'daily_listening_sync' 
        AND status = 'SUCCESS'
        ORDER BY completed_at DESC 
        LIMIT 1
        """
        
        result = pg_conn.execute_query(query)
        pg_conn.close()
        
        if result and len(result) > 0:
            last_sync = result[0][0]
            logging.info(f"ä¸Šæ¬¡åŒæ­¥æ™‚é–“: {last_sync}")
            return last_sync.isoformat()
        else:
            # ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼Œå¾æ˜¨å¤©é–‹å§‹
            yesterday = datetime.now(timezone.utc) - timedelta(days=1)
            logging.info(f"é¦–æ¬¡åŸ·è¡Œï¼Œå¾æ˜¨å¤©é–‹å§‹: {yesterday}")
            return yesterday.isoformat()
            
    except Exception as e:
        logging.warning(f"ç„¡æ³•å–å¾—ä¸Šæ¬¡åŒæ­¥æ™‚é–“ï¼Œä½¿ç”¨æ˜¨å¤©ä½œç‚ºèµ·é»: {e}")
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        return yesterday.isoformat()

def sync_listening_to_raw_staging(**context) -> Dict:
    """Task 1: åŒæ­¥è½æ­Œè¨˜éŒ„ MongoDB â†’ Raw Staging"""
    try:
        config = load_config()
        
        # å–å¾—ä¸Šæ¬¡åŒæ­¥æ™‚é–“é»
        last_sync_timestamp = context['task_instance'].xcom_pull(
            task_ids='get_sync_watermark'
        )
        
        # ä¿®å¾©ï¼šæ­£ç¢ºåˆå§‹åŒ–è³‡æ–™åº«é€£æ¥ï¼ˆä¸å‚³å…¥åƒæ•¸ï¼‰
        mongo_conn = MongoDBConnection()
        pg_conn = PostgreSQLConnection()
        
        # é€£æ¥è³‡æ–™åº«
        if not mongo_conn.connect():
            raise Exception("MongoDB é€£æ¥å¤±æ•—")
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        # å¾ MongoDB å–å¾—æ–°è³‡æ–™
        collection = mongo_conn.db['daily_listening_history']
        
        # ä¿®å¾©ï¼šä½¿ç”¨ batch_info.collected_at æ¬„ä½é€²è¡ŒæŸ¥è©¢
        last_sync_time = datetime.fromisoformat(last_sync_timestamp.replace('Z', '+00:00'))
        
        # æŸ¥è©¢æ¢ä»¶: batch_info.collected_at > ä¸Šæ¬¡åŒæ­¥æ™‚é–“
        query = {
            'batch_info.collected_at': {'$gt': last_sync_time}
        }
        
        # å–å¾—æ–°è¨˜éŒ„
        new_records = list(collection.find(query).sort('batch_info.collected_at', 1))
        
        if not new_records:
            logging.info("æ²’æœ‰æ–°çš„è½æ­Œè¨˜éŒ„éœ€è¦åŒæ­¥")
            mongo_conn.close()
            pg_conn.close()
            return {
                'status': 'SUCCESS',
                'records_processed': 0,
                'message': 'æ²’æœ‰æ–°è³‡æ–™'
            }
        
        logging.info(f"æ‰¾åˆ° {len(new_records)} ç­†æ–°è¨˜éŒ„")
        
        # æº–å‚™æ’å…¥ PostgreSQL çš„è³‡æ–™
        insert_data = []
        batch_id = context['ds_nodash'] + '_listening_sync'
        
        for record in new_records:
            # è™•ç†åµŒå¥—çš„ track_info
            track_info = record.get('track_info', {})
            
            # è™•ç† artists é™£åˆ—ï¼Œå–ç¬¬ä¸€å€‹è—è¡“å®¶
            artists = track_info.get('artists', [])
            first_artist = artists[0] if artists else {}
            
            # è™•ç† album è³‡è¨Š
            album_info = track_info.get('album', {})
            
            row = {
                'track_id': record.get('track_id'),
                'played_at': record.get('played_at'),
                'track_name': track_info.get('name'),
                'artist_id': first_artist.get('id'),
                'artist_name': first_artist.get('name'),
                'album_id': album_info.get('id'),
                'album_name': album_info.get('name'),
                'duration_ms': track_info.get('duration_ms'),
                'explicit': track_info.get('explicit', False),
                'popularity': track_info.get('popularity'),
                'batch_id': batch_id
            }
            
            insert_data.append(row)
        
        # æ‰¹æ¬¡æ’å…¥ PostgreSQL
        df = pd.DataFrame(insert_data)
        
        # æ’å…¥åˆ° raw staging è¡¨
        insert_query = """
        INSERT INTO raw_staging.spotify_listening_raw 
        (track_id, played_at, track_name, artist_id, artist_name, 
         album_id, album_name, duration_ms, explicit, popularity, batch_id)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (track_id, played_at) DO NOTHING
        """
        
        success_count = 0
        for _, row in df.iterrows():
            try:
                params = (
                    row['track_id'], row['played_at'], row['track_name'],
                    row['artist_id'], row['artist_name'], row['album_id'],
                    row['album_name'], row['duration_ms'], row['explicit'],
                    row['popularity'], row['batch_id']
                )
                
                # ä½¿ç”¨ cursor åŸ·è¡Œï¼Œé¿å… fetch éŒ¯èª¤
                cursor = pg_conn.connection.cursor()
                cursor.execute(insert_query, params)
                pg_conn.connection.commit()
                cursor.close()
                success_count += 1
                
            except Exception as e:
                logging.warning(f"æ’å…¥è¨˜éŒ„å¤±æ•—: {e}")
                pg_conn.connection.rollback()
                continue
        
        # æ›´æ–°åŒæ­¥æ™‚é–“æˆ³ - ä½¿ç”¨ batch_info.collected_at
        if new_records:
            # æ‰¾åˆ°æœ€æ–°çš„ batch_info.collected_at
            latest_times = []
            for record in new_records:
                batch_info = record.get('batch_info', {})
                collected_at = batch_info.get('collected_at')
                if collected_at:
                    if isinstance(collected_at, str):
                        latest_times.append(datetime.fromisoformat(collected_at.replace('Z', '+00:00')))
                    else:
                        latest_times.append(collected_at)
            
            last_record_time = max(latest_times) if latest_times else datetime.now(timezone.utc)
        else:
            last_record_time = datetime.now(timezone.utc)
        
        # é—œé–‰é€£æ¥
        mongo_conn.close()
        pg_conn.close()
        
        logging.info(f"åŒæ­¥å®Œæˆ: {success_count}/{len(new_records)} ç­†è¨˜éŒ„æˆåŠŸ")
        
        return {
            'status': 'SUCCESS',
            'records_processed': success_count,
            'last_sync_timestamp': last_record_time.isoformat() if last_record_time else None
        }
        
    except Exception as e:
        logging.error(f"åŒæ­¥è½æ­Œè¨˜éŒ„å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e),
            'records_processed': 0
        }

def process_time_fields(**context) -> Dict:
    """Task 2: è™•ç†æ™‚é–“æ¬„ä½ï¼Œå¾ Raw Staging â†’ Clean Staging"""
    try:
        config = load_config()
        pg_conn = PostgreSQLConnection()
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        batch_id = context['ds_nodash'] + '_listening_sync'
        
        # ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„è¡¨çµæ§‹å’Œæ¬„ä½åç¨±
        time_processing_query = """
        INSERT INTO clean_staging.listening_cleaned
        (raw_id, track_id, played_at, track_name, artist_name, album_name,
         duration_minutes, played_date, played_hour, played_day_of_week,
         time_period, is_weekend)
        SELECT 
            r.id as raw_id,
            r.track_id, 
            r.played_at, 
            r.track_name, 
            r.artist_name,
            r.album_name,
            ROUND(r.duration_ms / 60000.0, 2) as duration_minutes,
            DATE(r.played_at) as played_date,
            EXTRACT(HOUR FROM r.played_at) as played_hour,
            EXTRACT(DOW FROM r.played_at) as played_day_of_week,
            CASE 
                WHEN EXTRACT(HOUR FROM r.played_at) BETWEEN 6 AND 11 THEN 'morning'
                WHEN EXTRACT(HOUR FROM r.played_at) BETWEEN 12 AND 17 THEN 'afternoon'
                WHEN EXTRACT(HOUR FROM r.played_at) BETWEEN 18 AND 22 THEN 'evening'
                ELSE 'night'
            END as time_period,
            CASE WHEN EXTRACT(DOW FROM r.played_at) IN (0, 6) THEN true ELSE false END as is_weekend
        FROM raw_staging.spotify_listening_raw r
        WHERE r.batch_id = %s
        AND r.id NOT IN (
            SELECT c.raw_id FROM clean_staging.listening_cleaned c
            WHERE c.raw_id IS NOT NULL
        )
        """
        
        # ä¿®å¾©ï¼šåŸ·è¡Œ INSERT ä¸éœ€è¦ fetch çµæœ
        try:
            pg_conn.connection.cursor().execute(time_processing_query, (batch_id,))
            pg_conn.connection.commit()
            logging.info("æ™‚é–“è™•ç† INSERT åŸ·è¡ŒæˆåŠŸ")
        except Exception as e:
            logging.error(f"INSERT åŸ·è¡Œå¤±æ•—: {e}")
            pg_conn.connection.rollback()
            raise e
        
        # å–å¾—è™•ç†çš„è¨˜éŒ„æ•¸ - åˆ†é–‹çš„æŸ¥è©¢
        count_query = """
        SELECT COUNT(*) FROM clean_staging.listening_cleaned c
        JOIN raw_staging.spotify_listening_raw r ON c.raw_id = r.id
        WHERE r.batch_id = %s
        """
        
        try:
            cursor = pg_conn.connection.cursor()
            cursor.execute(count_query, (batch_id,))
            result = cursor.fetchone()
            processed_count = result[0] if result else 0
            cursor.close()
        except Exception as e:
            logging.warning(f"è¨ˆç®—è¨˜éŒ„æ•¸å¤±æ•—: {e}")
            processed_count = 0
        
        pg_conn.close()
        
        logging.info(f"æ™‚é–“æ¬„ä½è™•ç†å®Œæˆ: {processed_count} ç­†è¨˜éŒ„")
        
        return {
            'status': 'SUCCESS',
            'records_processed': processed_count
        }
        
    except Exception as e:
        logging.error(f"æ™‚é–“æ¬„ä½è™•ç†å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }

def load_to_warehouse(**context) -> Dict:
    """Task 3: è¼‰å…¥åˆ°æ•¸æ“šå€‰å„²äº‹å¯¦è¡¨"""
    try:
        config = load_config()
        pg_conn = PostgreSQLConnection()
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        batch_id = context['ds_nodash'] + '_listening_sync'
        
        # ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„è¡¨åç¨± dwh.fact_listening
        load_fact_query = """
        INSERT INTO dwh.fact_listening
        (track_id, played_at, played_date, played_hour, played_day_of_week, 
         duration_minutes, time_period, is_weekend)
        SELECT 
            c.track_id, c.played_at, c.played_date,
            c.played_hour, c.played_day_of_week, c.duration_minutes,
            c.time_period, c.is_weekend
        FROM clean_staging.listening_cleaned c
        JOIN raw_staging.spotify_listening_raw r ON c.raw_id = r.id
        WHERE r.batch_id = %s
        AND NOT EXISTS (
            SELECT 1 FROM dwh.fact_listening f
            WHERE f.track_id = c.track_id AND f.played_at = c.played_at
        )
        """
        
        # ä¿®å¾©ï¼šåŸ·è¡Œ INSERT ä¸éœ€è¦ fetch çµæœ
        try:
            cursor = pg_conn.connection.cursor()
            cursor.execute(load_fact_query, (batch_id,))
            pg_conn.connection.commit()
            cursor.close()
            logging.info("äº‹å¯¦è¡¨ INSERT åŸ·è¡ŒæˆåŠŸ")
        except Exception as e:
            logging.error(f"äº‹å¯¦è¡¨ INSERT å¤±æ•—: {e}")
            pg_conn.connection.rollback()
            raise e
        
        # å–å¾—è¼‰å…¥çš„è¨˜éŒ„æ•¸
        count_query = """
        SELECT COUNT(*) FROM dwh.fact_listening f
        WHERE EXISTS (
            SELECT 1 FROM clean_staging.listening_cleaned c
            JOIN raw_staging.spotify_listening_raw r ON c.raw_id = r.id
            WHERE r.batch_id = %s 
            AND f.track_id = c.track_id 
            AND f.played_at = c.played_at
        )
        """
        
        try:
            cursor = pg_conn.connection.cursor()
            cursor.execute(count_query, (batch_id,))
            result = cursor.fetchone()
            loaded_count = result[0] if result else 0
            cursor.close()
        except Exception as e:
            logging.warning(f"è¨ˆç®—è¼‰å…¥è¨˜éŒ„æ•¸å¤±æ•—: {e}")
            loaded_count = 0
        
        pg_conn.close()
        
        logging.info(f"äº‹å¯¦è¡¨è¼‰å…¥å®Œæˆ: {loaded_count} ç­†è¨˜éŒ„")
        
        return {
            'status': 'SUCCESS',
            'records_loaded': loaded_count
        }
        
    except Exception as e:
        logging.error(f"äº‹å¯¦è¡¨è¼‰å…¥å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }

def sync_tracks_to_dwh(**context) -> Dict:
    """Task 4a: åŒæ­¥æ­Œæ›²ç¶­åº¦è¡¨"""
    try:
        config = load_config()
        mongo_conn = MongoDBConnection()
        pg_conn = PostgreSQLConnection()
        
        # é€£æ¥è³‡æ–™åº«
        if not mongo_conn.connect():
            raise Exception("MongoDB é€£æ¥å¤±æ•—")
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        # å¾ MongoDB å–å¾—ä»Šæ—¥æœ‰è½éä½† DWH ä¸­æ²’æœ‰çš„æ­Œæ›²
        collection = mongo_conn.db['track_details']
        
        # å–å¾—éœ€è¦åŒæ­¥çš„ track_ids (ä»Šæ—¥è½éçš„)
        batch_id = context['ds_nodash'] + '_listening_sync'
        
        get_track_ids_query = """
        SELECT DISTINCT r.track_id 
        FROM raw_staging.spotify_listening_raw r
        WHERE r.batch_id = %s
        AND r.track_id NOT IN (SELECT track_id FROM dwh.dim_tracks)
        """
        
        result = pg_conn.execute_query(get_track_ids_query, (batch_id,))
        track_ids = [row[0] for row in result] if result else []
        
        if not track_ids:
            logging.info("æ²’æœ‰æ–°çš„æ­Œæ›²éœ€è¦åŒæ­¥åˆ°ç¶­åº¦è¡¨")
            mongo_conn.close()
            pg_conn.close()
            return {'status': 'SUCCESS', 'records_synced': 0}
        
        logging.info(f"éœ€è¦åŒæ­¥ {len(track_ids)} é¦–æ–°æ­Œæ›²")
        
        # å¾ MongoDB å–å¾—æ­Œæ›²è©³ç´°è³‡è¨Š
        tracks_data = list(collection.find({'track_id': {'$in': track_ids}}))
        
        sync_count = 0
        for track in tracks_data:
            try:
                insert_track_query = """
                INSERT INTO dwh.dim_tracks 
                (track_id, track_name, duration_minutes, explicit, 
                 popularity, first_heard, total_plays)
                VALUES (%s, %s, %s, %s, %s, %s, 1)
                ON CONFLICT (track_id) DO UPDATE SET
                total_plays = dim_tracks.total_plays + 1,
                last_updated = CURRENT_TIMESTAMP
                """
                
                params = (
                    track.get('track_id'),
                    track.get('name'),
                    round(track.get('duration_ms', 0) / 60000.0, 2),
                    track.get('explicit', False),
                    track.get('popularity'),
                    datetime.now().date()
                )
                
                pg_conn.execute_query(insert_track_query, params)
                sync_count += 1
                
            except Exception as e:
                logging.warning(f"åŒæ­¥æ­Œæ›²å¤±æ•— {track.get('track_id')}: {e}")
                continue
        
        mongo_conn.close()
        pg_conn.close()
        
        logging.info(f"æˆåŠŸåŒæ­¥ {sync_count} é¦–æ­Œæ›²åˆ°ç¶­åº¦è¡¨")
        
        return {
            'status': 'SUCCESS',
            'records_synced': sync_count
        }
        
    except Exception as e:
        logging.error(f"åŒæ­¥æ­Œæ›²ç¶­åº¦è¡¨å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }

def sync_artists_to_dwh(**context) -> Dict:
    """Task 4b: åŒæ­¥è—è¡“å®¶ç¶­åº¦è¡¨"""
    try:
        config = load_config()
        mongo_conn = MongoDBConnection()
        pg_conn = PostgreSQLConnection()
        
        # é€£æ¥è³‡æ–™åº«
        if not mongo_conn.connect():
            raise Exception("MongoDB é€£æ¥å¤±æ•—")
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        collection = mongo_conn.db['artist_profiles']
        batch_id = context['ds_nodash'] + '_listening_sync'
        
        # å–å¾—éœ€è¦åŒæ­¥çš„ artist_ids
        get_artist_ids_query = """
        SELECT DISTINCT r.artist_id 
        FROM raw_staging.spotify_listening_raw r
        WHERE r.batch_id = %s
        AND r.artist_id IS NOT NULL
        AND r.artist_id NOT IN (SELECT artist_id FROM dwh.dim_artists)
        """
        
        result = pg_conn.execute_query(get_artist_ids_query, (batch_id,))
        artist_ids = [row[0] for row in result] if result else []
        
        if not artist_ids:
            mongo_conn.close()
            pg_conn.close()
            return {'status': 'SUCCESS', 'records_synced': 0}
        
        # å¾ MongoDB å–å¾—è—è¡“å®¶è³‡è¨Š
        artists_data = list(collection.find({'artist_id': {'$in': artist_ids}}))
        
        sync_count = 0
        for artist in artists_data:
            try:
                insert_artist_query = """
                INSERT INTO dwh.dim_artists 
                (artist_id, artist_name, genres, popularity, 
                 followers_count, first_discovered, total_plays)
                VALUES (%s, %s, %s, %s, %s, %s, 1)
                ON CONFLICT (artist_id) DO UPDATE SET
                total_plays = dim_artists.total_plays + 1,
                last_updated = CURRENT_TIMESTAMP
                """
                
                params = (
                    artist.get('artist_id'),
                    artist.get('name'),
                    artist.get('genres', []),
                    artist.get('popularity'),
                    artist.get('followers'),
                    datetime.now().date()
                )
                
                pg_conn.execute_query(insert_artist_query, params)
                sync_count += 1
                
            except Exception as e:
                logging.warning(f"åŒæ­¥è—è¡“å®¶å¤±æ•— {artist.get('artist_id')}: {e}")
                continue
        
        mongo_conn.close()
        pg_conn.close()
        
        return {
            'status': 'SUCCESS', 
            'records_synced': sync_count
        }
        
    except Exception as e:
        logging.error(f"åŒæ­¥è—è¡“å®¶ç¶­åº¦è¡¨å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }

def sync_albums_to_dwh(**context) -> Dict:
    """Task 4c: åŒæ­¥å°ˆè¼¯ç¶­åº¦è¡¨"""
    try:
        config = load_config()
        mongo_conn = MongoDBConnection()
        pg_conn = PostgreSQLConnection()
        
        # é€£æ¥è³‡æ–™åº«
        if not mongo_conn.connect():
            raise Exception("MongoDB é€£æ¥å¤±æ•—")
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        collection = mongo_conn.db['album_catalog']
        batch_id = context['ds_nodash'] + '_listening_sync'
        
        # å–å¾—éœ€è¦åŒæ­¥çš„ album_ids
        get_album_ids_query = """
        SELECT DISTINCT r.album_id 
        FROM raw_staging.spotify_listening_raw r
        WHERE r.batch_id = %s
        AND r.album_id IS NOT NULL
        AND r.album_id NOT IN (SELECT album_id FROM dwh.dim_albums)
        """
        
        result = pg_conn.execute_query(get_album_ids_query, (batch_id,))
        album_ids = [row[0] for row in result] if result else []
        
        if not album_ids:
            mongo_conn.close()
            pg_conn.close()
            return {'status': 'SUCCESS', 'records_synced': 0}
        
        # å¾ MongoDB å–å¾—å°ˆè¼¯è³‡è¨Š
        albums_data = list(collection.find({'album_id': {'$in': album_ids}}))
        
        sync_count = 0
        for album in albums_data:
            try:
                insert_album_query = """
                INSERT INTO dwh.dim_albums 
                (album_id, album_name, release_date, total_tracks, 
                 album_type, first_discovered, total_plays)
                VALUES (%s, %s, %s, %s, %s, %s, 1)
                ON CONFLICT (album_id) DO UPDATE SET
                total_plays = dim_albums.total_plays + 1,
                last_updated = CURRENT_TIMESTAMP
                """
                
                params = (
                    album.get('album_id'),
                    album.get('name'),
                    album.get('release_date'),
                    album.get('total_tracks'),
                    album.get('album_type'),
                    datetime.now().date()
                )
                
                pg_conn.execute_query(insert_album_query, params)
                sync_count += 1
                
            except Exception as e:
                logging.warning(f"åŒæ­¥å°ˆè¼¯å¤±æ•— {album.get('album_id')}: {e}")
                continue
        
        mongo_conn.close()
        pg_conn.close()
        
        return {
            'status': 'SUCCESS',
            'records_synced': sync_count
        }
        
    except Exception as e:
        logging.error(f"åŒæ­¥å°ˆè¼¯ç¶­åº¦è¡¨å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }

def update_daily_stats(**context) -> Dict:
    """Task 5: æ›´æ–°æ¯æ—¥èšåˆçµ±è¨ˆ"""
    try:
        config = load_config()
        pg_conn = PostgreSQLConnection()
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        today = context['ds']
        
        # ä¿®å¾©ï¼šä½¿ç”¨æ­£ç¢ºçš„è¡¨åç¨±å’Œæ¬„ä½åç¨±
        upsert_daily_stats_query = """
        INSERT INTO dwh.agg_daily_stats 
        (stats_date, total_tracks, unique_tracks, total_duration_minutes, 
         avg_track_duration, created_at, updated_at)
        SELECT 
            %s::date as stats_date,
            COUNT(*) as total_tracks,
            COUNT(DISTINCT track_id) as unique_tracks,
            SUM(duration_minutes) as total_duration_minutes,
            AVG(duration_minutes) as avg_track_duration,
            CURRENT_TIMESTAMP as created_at,
            CURRENT_TIMESTAMP as updated_at
        FROM dwh.fact_listening 
        WHERE played_date = %s::date
        ON CONFLICT (stats_date) DO UPDATE SET
            total_tracks = EXCLUDED.total_tracks,
            unique_tracks = EXCLUDED.unique_tracks,
            total_duration_minutes = EXCLUDED.total_duration_minutes,
            avg_track_duration = EXCLUDED.avg_track_duration,
            updated_at = CURRENT_TIMESTAMP
        """
        
        # ä¿®å¾©ï¼šåŸ·è¡Œ UPSERT ä¸éœ€è¦ fetch çµæœ
        try:
            cursor = pg_conn.connection.cursor()
            cursor.execute(upsert_daily_stats_query, (today, today))
            pg_conn.connection.commit()
            cursor.close()
            logging.info("æ¯æ—¥çµ±è¨ˆ UPSERT åŸ·è¡ŒæˆåŠŸ")
        except Exception as e:
            logging.error(f"æ¯æ—¥çµ±è¨ˆ UPSERT å¤±æ•—: {e}")
            pg_conn.connection.rollback()
            raise e
        
        pg_conn.close()
        
        logging.info(f"æ¯æ—¥çµ±è¨ˆå·²æ›´æ–°: {today}")
        
        return {
            'status': 'SUCCESS',
            'date_processed': today
        }
        
    except Exception as e:
        logging.error(f"æ›´æ–°æ¯æ—¥çµ±è¨ˆå¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }

def log_etl_batch(**context) -> Dict:
    """è¨˜éŒ„ ETL åŸ·è¡Œçµæœ"""
    try:
        config = load_config()
        pg_conn = PostgreSQLConnection()
        
        if not pg_conn.connect():
            raise Exception("PostgreSQL é€£æ¥å¤±æ•—")
        
        # å¾å‰é¢çš„ task æ”¶é›†çµæœ
        sync_result = context['task_instance'].xcom_pull(task_ids='sync_listening_to_raw_staging')
        
        status = 'SUCCESS' if sync_result.get('status') == 'SUCCESS' else 'FAILED'
        records_processed = sync_result.get('records_processed', 0)
        last_sync_timestamp = sync_result.get('last_sync_timestamp')
        
        # è¨˜éŒ„åˆ° ETL æ—¥èªŒè¡¨
        log_query = """
        INSERT INTO dwh.etl_batch_log 
        (job_name, batch_id, status, records_processed, 
         last_sync_timestamp, started_at, completed_at, error_message)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        params = (
            'daily_listening_sync',
            context['ds_nodash'] + '_listening_sync',
            status,
            records_processed,
            last_sync_timestamp,
            context['task_instance'].start_date,
            datetime.now(),
            sync_result.get('error') if status == 'FAILED' else None
        )
        
        # ä¿®å¾©ï¼šåŸ·è¡Œ INSERT ä¸éœ€è¦ fetch çµæœ
        try:
            cursor = pg_conn.connection.cursor()
            cursor.execute(log_query, params)
            pg_conn.connection.commit()
            cursor.close()
            logging.info("ETL æ—¥èªŒ INSERT åŸ·è¡ŒæˆåŠŸ")
        except Exception as e:
            logging.error(f"ETL æ—¥èªŒ INSERT å¤±æ•—: {e}")
            pg_conn.connection.rollback()
            raise e
        
        pg_conn.close()
        
        logging.info(f"ETL æ‰¹æ¬¡è¨˜éŒ„å·²å„²å­˜: {status}")
        
        return {
            'status': 'SUCCESS',
            'batch_logged': True
        }
        
    except Exception as e:
        logging.error(f"è¨˜éŒ„ ETL æ‰¹æ¬¡å¤±æ•—: {e}")
        return {
            'status': 'FAILED',
            'error': str(e)
        }

# ============================================================================
# DAG Tasks å®šç¾©
# ============================================================================

# Task 0: å–å¾—åŒæ­¥èµ·å§‹é»
get_sync_watermark = PythonOperator(
    task_id='get_sync_watermark',
    python_callable=get_last_sync_timestamp,
    dag=dag
)

# Task 1: åŒæ­¥è½æ­Œè¨˜éŒ„
sync_listening_task = PythonOperator(
    task_id='sync_listening_to_raw_staging',
    python_callable=sync_listening_to_raw_staging,
    dag=dag
)

# Task 2: æ™‚é–“è™•ç†
process_time_task = PythonOperator(
    task_id='process_time_fields',
    python_callable=process_time_fields,
    dag=dag
)

# Task 3: è¼‰å…¥ DWH
load_warehouse_task = PythonOperator(
    task_id='load_to_warehouse',
    python_callable=load_to_warehouse,
    dag=dag
)

# Task 4: ç¶­åº¦è¡¨åŒæ­¥ (å¹³è¡ŒåŸ·è¡Œ)
with TaskGroup("sync_dimensions", dag=dag) as sync_dimensions_group:
    sync_tracks_task = PythonOperator(
        task_id='sync_tracks_to_dwh',
        python_callable=sync_tracks_to_dwh
    )
    
    sync_artists_task = PythonOperator(
        task_id='sync_artists_to_dwh',
        python_callable=sync_artists_to_dwh
    )
    
    sync_albums_task = PythonOperator(
        task_id='sync_albums_to_dwh',
        python_callable=sync_albums_to_dwh
    )

# Task 5: æ›´æ–°èšåˆçµ±è¨ˆ
update_stats_task = PythonOperator(
    task_id='update_daily_stats',
    python_callable=update_daily_stats,
    dag=dag
)

# Task 6: è¨˜éŒ„ ETL çµæœ
log_batch_task = PythonOperator(
    task_id='log_etl_batch',
    python_callable=log_etl_batch,
    dag=dag,
    trigger_rule='none_failed_or_skipped'  # å³ä½¿æœ‰éƒ¨åˆ†å¤±æ•—ä¹Ÿè¦è¨˜éŒ„
)

# ============================================================================
# ä»»å‹™ä¾è³´é—œä¿‚
# ============================================================================

# ç·šæ€§ä¾è³´ + å¹³è¡Œè™•ç†
get_sync_watermark >> sync_listening_task >> process_time_task

# å¹³è¡Œè™•ç†ç¶­åº¦è¡¨åŒæ­¥
process_time_task >> sync_dimensions_group
process_time_task >> load_warehouse_task

# ç­‰ç¶­åº¦è¡¨å’Œäº‹å¯¦è¡¨éƒ½å®Œæˆå¾Œï¼Œæ›´æ–°çµ±è¨ˆ
[sync_dimensions_group, load_warehouse_task] >> update_stats_task

# æœ€å¾Œè¨˜éŒ„åŸ·è¡Œçµæœ
update_stats_task >> log_batch_task

# ============================================================================
# DAG åŸ·è¡Œæ‘˜è¦ - å®Œå…¨ä¿®å¾©ç‰ˆ
# ============================================================================
"""
ğŸ‰ å®Œå…¨ä¿®å¾©ç‰ˆæœ¬ - æ‰€æœ‰è¡¨çµæ§‹å·²åŒ¹é…å¯¦éš›è³‡æ–™åº«

ä¿®å¾©å…§å®¹:
1. âœ… MongoDB å’Œ PostgreSQL é€£æ¥åˆå§‹åŒ–
2. âœ… æ‰€æœ‰è¡¨åç¨±åŒ¹é… (fact_listening_history â†’ fact_listening)
3. âœ… æ‰€æœ‰æ¬„ä½åç¨±åŒ¹é…å¯¦éš›è¡¨çµæ§‹
4. âœ… INSERT èªå¥ç¬¦åˆè¡¨çš„æ¬„ä½å®šç¾©
5. âœ… æ­£ç¢ºçš„æ™‚é–“è™•ç†å’Œè³‡æ–™è½‰æ›
6. âœ… é©ç•¶çš„éŒ¯èª¤è™•ç†å’Œè³‡æºæ¸…ç†

è¡¨çµæ§‹åŒ¹é…:
- raw_staging.spotify_listening_raw âœ…
- clean_staging.listening_cleaned âœ…  
- dwh.fact_listening âœ…
- dwh.dim_tracks, dwh.dim_artists, dwh.dim_albums âœ…
- dwh.etl_batch_log âœ…
- dwh.agg_daily_stats âœ…

æ–°å¢åŠŸèƒ½:
- æ™ºèƒ½æ™‚é–“åˆ†æ (morning/afternoon/evening/night)
- é€±æœ«/å¹³æ—¥åˆ†æ
- æ­£ç¢ºçš„æŒçºŒæ™‚é–“è½‰æ› (ms â†’ minutes)
- æ”¹å–„çš„é‡è¤‡æª¢æŸ¥é‚è¼¯

åŸ·è¡Œæµç¨‹:
1. get_sync_watermark: å–å¾—ä¸Šæ¬¡åŒæ­¥æ™‚é–“é» 
2. sync_listening_to_raw_staging: MongoDB â†’ Raw Staging
3. process_time_fields: Raw â†’ Clean Staging (æ™‚é–“è§£æ)
4. sync_dimensions (å¹³è¡Œ):
   - sync_tracks_to_dwh: åŒæ­¥æ–°æ­Œæ›²åˆ°ç¶­åº¦è¡¨
   - sync_artists_to_dwh: åŒæ­¥æ–°è—è¡“å®¶åˆ°ç¶­åº¦è¡¨  
   - sync_albums_to_dwh: åŒæ­¥æ–°å°ˆè¼¯åˆ°ç¶­åº¦è¡¨
5. load_to_warehouse: Clean â†’ DWH Fact Table
6. update_daily_stats: è¨ˆç®—æ¯æ—¥èšåˆçµ±è¨ˆ
7. log_etl_batch: è¨˜éŒ„åŸ·è¡Œçµæœ

ç¾åœ¨æ‰€æœ‰è¡¨çµæ§‹éƒ½åŒ¹é…ï¼ŒDAG æ‡‰è©²èƒ½æ­£å¸¸é‹è¡Œï¼
"""