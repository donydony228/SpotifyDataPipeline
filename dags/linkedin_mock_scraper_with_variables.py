# dags/linkedin_mock_scraper_with_variables.py
# ä½¿ç”¨ Airflow Variables è§£æ±ºç’°å¢ƒè®Šæ•¸å•é¡Œ

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
import random
import json
import uuid

# ============================================================================
# DAG é…ç½®
# ============================================================================

default_args = {
    'owner': 'data-engineering-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

dag = DAG(
    'linkedin_mock_scraper_with_variables',
    default_args=default_args,
    description='ğŸ¯ LinkedIn æ¨¡æ“¬çˆ¬èŸ² - ä½¿ç”¨ Airflow Variables',
    schedule=None,
    catchup=False,
    tags=['scraper', 'linkedin', 'mock', 'variables']
)

# ============================================================================
# Mock çˆ¬èŸ²
# ============================================================================

class MockLinkedInScraper:
    """æ¨¡æ“¬ LinkedIn çˆ¬èŸ²"""
    
    def __init__(self, target_jobs=10):
        self.target_jobs = target_jobs
        self.job_titles = [
            'Senior Data Engineer', 'Data Engineer', 'ML Engineer',
            'Data Scientist', 'Analytics Engineer', 'Backend Engineer'
        ]
        self.companies = [
            'Tech Corp', 'Data Inc', 'AI Solutions', 'Cloud Systems',
            'Innovation Labs', 'Digital Ventures'
        ]
        self.locations = [
            'San Francisco, CA', 'New York, NY', 'Seattle, WA',
            'Austin, TX', 'Boston, MA', 'Remote'
        ]
    
    def scrape(self):
        """ç”Ÿæˆæ¨¡æ“¬è·ç¼ºæ•¸æ“š"""
        jobs = []
        for i in range(self.target_jobs):
            job = {
                'job_id': f'mock_linkedin_{uuid.uuid4().hex[:8]}',
                'title': random.choice(self.job_titles),
                'company': random.choice(self.companies),
                'location': random.choice(self.locations),
                'salary_min': random.randint(80000, 150000),
                'salary_max': random.randint(150000, 250000),
                'posted_date': datetime.now().isoformat(),
                'description': f'Exciting opportunity for {random.choice(self.job_titles)}',
                'source': 'linkedin',
                'is_mock': True
            }
            jobs.append(job)
        return jobs

# ============================================================================
# Task å‡½æ•¸
# ============================================================================

def check_variables(**context):
    """æª¢æŸ¥ Airflow Variables æ˜¯å¦è¨­ç½®"""
    print("ğŸ” æª¢æŸ¥ Airflow Variables...")
    
    required_vars = {
        'SUPABASE_DB_URL': 'Supabase PostgreSQL é€£æ¥å­—ä¸²',
        'MONGODB_ATLAS_URL': 'MongoDB Atlas é€£æ¥å­—ä¸²',
        'MONGODB_ATLAS_DB_NAME': 'MongoDB è³‡æ–™åº«åç¨±'
    }
    
    results = {}
    all_found = True
    
    for var_name, description in required_vars.items():
        try:
            value = Variable.get(var_name)
            if value:
                # åªé¡¯ç¤ºå‰ 40 å€‹å­—ç¬¦
                masked = f"{value[:40]}..." if len(value) > 40 else value
                print(f"  âœ… {var_name}: {masked}")
                results[var_name] = 'found'
            else:
                print(f"  âŒ {var_name}: è¨­ç½®ä½†ç‚ºç©º")
                results[var_name] = 'empty'
                all_found = False
        except Exception as e:
            print(f"  âŒ {var_name}: æœªè¨­ç½® ({description})")
            results[var_name] = 'missing'
            all_found = False
    
    if not all_found:
        print("\nâš ï¸  è­¦å‘Šï¼šéƒ¨åˆ†è®Šæ•¸ç¼ºå¤±ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼")
        print("ğŸ’¡ æç¤ºï¼šé‹è¡Œ ./fix_supabase_connection.sh ä¾†è¨­ç½® Airflow Variables")
    
    context['task_instance'].xcom_push(key='variables_check', value=results)
    return results

def scrape_jobs(**context):
    """çˆ¬å–ï¼ˆç”Ÿæˆï¼‰è·ç¼ºæ•¸æ“š"""
    print("ğŸ¯ é–‹å§‹æ¨¡æ“¬çˆ¬å–...")
    
    scraper = MockLinkedInScraper(target_jobs=10)
    jobs_data = scraper.scrape()
    
    batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    result = {
        'batch_id': batch_id,
        'jobs': jobs_data,
        'total_jobs': len(jobs_data),
        'timestamp': datetime.now().isoformat()
    }
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(jobs_data)} å€‹æ¨¡æ“¬è·ç¼º")
    
    context['task_instance'].xcom_push(key='scrape_result', value=result)
    return f"Scraped {len(jobs_data)} jobs"

def store_to_mongodb(**context):
    """å­˜å„²åˆ° MongoDB"""
    scrape_result = context['task_instance'].xcom_pull(
        task_ids='scrape_jobs',
        key='scrape_result'
    )
    
    jobs_data = scrape_result['jobs']
    batch_id = scrape_result['batch_id']
    
    print(f"ğŸƒ é–‹å§‹å­˜å„² {len(jobs_data)} å€‹è·ç¼ºåˆ° MongoDB...")
    
    # å˜—è©¦å¾ Airflow Variables ç²å–é€£æ¥è³‡è¨Š
    try:
        mongodb_url = Variable.get('MONGODB_ATLAS_URL')
        db_name = Variable.get('MONGODB_ATLAS_DB_NAME')
        
        print(f"âœ… å¾ Airflow Variables ç²å– MongoDB é€£æ¥è³‡è¨Š")
        print(f"   URL: {mongodb_url[:40]}...")
        print(f"   DB: {db_name}")
        
        # å¯¦éš›é€£æ¥ MongoDB
        from pymongo import MongoClient
        from pymongo.server_api import ServerApi
        
        client = MongoClient(mongodb_url, server_api=ServerApi('1'))
        db = client[db_name]
        collection = db['raw_jobs_data']
        
        # ç‚ºæ¯å€‹è·ç¼ºæ·»åŠ æ‰¹æ¬¡è³‡è¨Š
        for job in jobs_data:
            job['batch_id'] = batch_id
            job['stored_at'] = datetime.now().isoformat()
        
        # æ’å…¥æ•¸æ“š
        result = collection.insert_many(jobs_data)
        inserted_count = len(result.inserted_ids)
        
        client.close()
        
        print(f"âœ… æˆåŠŸå­˜å„² {inserted_count} å€‹è·ç¼ºåˆ° MongoDB Atlas")
        
        stats = {
            'inserted': inserted_count,
            'batch_id': batch_id,
            'collection': 'raw_jobs_data'
        }
        
        context['task_instance'].xcom_push(key='mongodb_stats', value=stats)
        return f"Stored {inserted_count} jobs to MongoDB"
        
    except Exception as e:
        print(f"âš ï¸  MongoDB é€£æ¥å¤±æ•—: {str(e)}")
        print("ğŸ’¡ æç¤ºï¼šç¢ºä¿å·²é‹è¡Œ ./fix_supabase_connection.sh")
        print("ğŸ“ æ¨¡æ“¬æ¨¡å¼ï¼šè¨˜éŒ„ä½†ä¸å¯¦éš›å­˜å„²")
        
        stats = {
            'inserted': 0,
            'batch_id': batch_id,
            'error': str(e),
            'simulated': True
        }
        
        context['task_instance'].xcom_push(key='mongodb_stats', value=stats)
        return f"MongoDB storage failed: {str(e)}"

def store_to_postgres(**context):
    """å­˜å„²åˆ° PostgreSQL (Supabase)"""
    scrape_result = context['task_instance'].xcom_pull(
        task_ids='scrape_jobs',
        key='scrape_result'
    )
    
    jobs_data = scrape_result['jobs']
    batch_id = scrape_result['batch_id']
    
    print(f"ğŸ˜ é–‹å§‹å­˜å„² {len(jobs_data)} å€‹è·ç¼ºåˆ° PostgreSQL...")
    
    # å˜—è©¦å¾ Airflow Variables ç²å–é€£æ¥è³‡è¨Š
    try:
        supabase_url = Variable.get('SUPABASE_DB_URL')
        
        print(f"âœ… å¾ Airflow Variables ç²å– Supabase é€£æ¥è³‡è¨Š")
        print(f"   URL: {supabase_url[:60]}...")
        
        # å¯¦éš›é€£æ¥ PostgreSQL
        import psycopg2
        import psycopg2.extras
        
        conn = psycopg2.connect(supabase_url)
        cur = conn.cursor()
        
        print("âœ… Supabase é€£æ¥æˆåŠŸ")
        
        # æ’å…¥æ•¸æ“šåˆ° raw_staging è¡¨
        inserted = 0
        for job in jobs_data:
            try:
                cur.execute("""
                    INSERT INTO raw_staging.linkedin_jobs_raw 
                    (job_data, source_url, batch_id, scraped_at)
                    VALUES (%s, %s, %s, %s)
                """, (
                    json.dumps(job),
                    'mock_scraper',
                    batch_id,
                    datetime.now()
                ))
                inserted += 1
            except Exception as e:
                print(f"âš ï¸  æ’å…¥å¤±æ•—: {str(e)}")
                continue
        
        conn.commit()
        cur.close()
        conn.close()
        
        print(f"âœ… æˆåŠŸå­˜å„² {inserted} å€‹è·ç¼ºåˆ° Supabase")
        
        stats = {
            'inserted': inserted,
            'batch_id': batch_id,
            'table': 'raw_staging.linkedin_jobs_raw'
        }
        
        context['task_instance'].xcom_push(key='postgres_stats', value=stats)
        return f"Stored {inserted} jobs to PostgreSQL"
        
    except Exception as e:
        print(f"âš ï¸  Supabase é€£æ¥å¤±æ•—: {str(e)}")
        print("ğŸ’¡ æç¤ºï¼š")
        print("   1. ç¢ºä¿å·²é‹è¡Œ ./fix_supabase_connection.sh")
        print("   2. æª¢æŸ¥ Supabase URL æ˜¯å¦æ­£ç¢ºï¼ˆä½¿ç”¨ pooler åœ°å€ï¼‰")
        print("   3. é©—è­‰ç¶²è·¯é€£æ¥")
        print("ğŸ“ æ¨¡æ“¬æ¨¡å¼ï¼šè¨˜éŒ„ä½†ä¸å¯¦éš›å­˜å„²")
        
        stats = {
            'inserted': 0,
            'batch_id': batch_id,
            'error': str(e),
            'simulated': True
        }
        
        context['task_instance'].xcom_push(key='postgres_stats', value=stats)
        return f"PostgreSQL storage failed: {str(e)}"

def log_summary(**context):
    """è¨˜éŒ„åŸ·è¡Œæ‘˜è¦"""
    print("ğŸ“Š åŸ·è¡Œæ‘˜è¦")
    print("=" * 60)
    
    # ç²å–å„å€‹æ­¥é©Ÿçš„çµæœ
    variables_check = context['task_instance'].xcom_pull(
        task_ids='check_variables',
        key='variables_check'
    )
    
    scrape_result = context['task_instance'].xcom_pull(
        task_ids='scrape_jobs',
        key='scrape_result'
    )
    
    mongodb_stats = context['task_instance'].xcom_pull(
        task_ids='store_to_mongodb',
        key='mongodb_stats'
    )
    
    postgres_stats = context['task_instance'].xcom_pull(
        task_ids='store_to_postgres',
        key='postgres_stats'
    )
    
    print(f"\nğŸ“‹ æ‰¹æ¬¡è³‡è¨Š:")
    print(f"   æ‰¹æ¬¡ ID: {scrape_result['batch_id']}")
    print(f"   æ™‚é–“: {scrape_result['timestamp']}")
    
    print(f"\nğŸ¯ çˆ¬å–çµæœ:")
    print(f"   ç”Ÿæˆè·ç¼º: {scrape_result['total_jobs']}")
    
    print(f"\nğŸƒ MongoDB å­˜å„²:")
    if mongodb_stats.get('simulated'):
        print(f"   âš ï¸  æ¨¡æ“¬æ¨¡å¼ï¼ˆæœªå¯¦éš›å­˜å„²ï¼‰")
        print(f"   éŒ¯èª¤: {mongodb_stats.get('error', 'N/A')}")
    else:
        print(f"   âœ… æˆåŠŸå­˜å„²: {mongodb_stats['inserted']} å€‹è·ç¼º")
    
    print(f"\nğŸ˜ PostgreSQL å­˜å„²:")
    if postgres_stats.get('simulated'):
        print(f"   âš ï¸  æ¨¡æ“¬æ¨¡å¼ï¼ˆæœªå¯¦éš›å­˜å„²ï¼‰")
        print(f"   éŒ¯èª¤: {postgres_stats.get('error', 'N/A')}")
    else:
        print(f"   âœ… æˆåŠŸå­˜å„²: {postgres_stats['inserted']} å€‹è·ç¼º")
    
    print("\n" + "=" * 60)
    
    return "Summary logged"

# ============================================================================
# å®šç¾© Tasks
# ============================================================================

check_vars_task = PythonOperator(
    task_id='check_variables',
    python_callable=check_variables,
    dag=dag
)

scrape_task = PythonOperator(
    task_id='scrape_jobs',
    python_callable=scrape_jobs,
    dag=dag
)

mongodb_task = PythonOperator(
    task_id='store_to_mongodb',
    python_callable=store_to_mongodb,
    dag=dag
)

postgres_task = PythonOperator(
    task_id='store_to_postgres',
    python_callable=store_to_postgres,
    dag=dag
)

summary_task = PythonOperator(
    task_id='log_summary',
    python_callable=log_summary,
    dag=dag
)

# ============================================================================
# å®šç¾©ä¾è³´é—œä¿‚
# ============================================================================

check_vars_task >> scrape_task >> [mongodb_task, postgres_task] >> summary_task