# dags/local_development_dag.py
# æœ¬åœ°é–‹ç™¼ç’°å¢ƒ - é›²ç«¯è³‡æ–™åº«é€£ç·šæ¸¬è©¦

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import sys
import os

# åŠ å…¥ src è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../src'))

def test_cloud_databases():
    """æ¸¬è©¦é›²ç«¯è³‡æ–™åº«é€£ç·š"""
    import psycopg2
    from pymongo import MongoClient
    from pymongo.server_api import ServerApi
    import os
    
    results = {}
    
    try:
        # æ¸¬è©¦ Supabase
        supabase_url = os.getenv('SUPABASE_DB_URL')
        if supabase_url:
            conn = psycopg2.connect(supabase_url)
            cur = conn.cursor()
            cur.execute("SELECT COUNT(*) FROM dwh.fact_jobs")
            job_count = cur.fetchone()[0]
            conn.close()
            results['supabase'] = f"âœ… Supabase: {job_count} jobs in warehouse"
            print(results['supabase'])
        else:
            results['supabase'] = "âŒ SUPABASE_DB_URL not configured"
            print(results['supabase'])
    
    except Exception as e:
        results['supabase'] = f"âŒ Supabase error: {str(e)}"
        print(results['supabase'])
    
    try:
        # æ¸¬è©¦ MongoDB Atlas
        mongodb_url = os.getenv('MONGODB_ATLAS_URL')
        mongodb_db = os.getenv('MONGODB_ATLAS_DB_NAME', 'job_market_data')
        
        if mongodb_url:
            client = MongoClient(mongodb_url, server_api=ServerApi('1'))
            db = client[mongodb_db]
            raw_count = db['raw_jobs_data'].count_documents({})
            client.close()
            results['mongodb'] = f"âœ… MongoDB Atlas: {raw_count} raw jobs"
            print(results['mongodb'])
        else:
            results['mongodb'] = "âŒ MONGODB_ATLAS_URL not configured"
            print(results['mongodb'])
    
    except Exception as e:
        results['mongodb'] = f"âŒ MongoDB Atlas error: {str(e)}"
        print(results['mongodb'])
    
    # æ¸¬è©¦æœ¬åœ°è³‡æ–™åº«
    try:
        conn = psycopg2.connect(
            host='localhost',
            port=5433,
            database='job_data_warehouse',
            user='dwh_user',
            password='dwh_password'
        )
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM dwh.fact_jobs")
        local_jobs = cur.fetchone()[0]
        conn.close()
        results['local_dwh'] = f"âœ… Local DWH: {local_jobs} jobs"
        print(results['local_dwh'])
    
    except Exception as e:
        results['local_dwh'] = f"âŒ Local DWH error: {str(e)}"
        print(results['local_dwh'])
    
    print("\nğŸ† æœ¬åœ°é–‹ç™¼ç’°å¢ƒç‹€æ…‹ç¸½çµ:")
    for db, status in results.items():
        print(f"  {db}: {status}")
    
    return results

def test_etl_pipeline():
    """æ¸¬è©¦å®Œæ•´ ETL Pipeline"""
    print("ğŸ”„ æ¸¬è©¦ ETL Pipeline...")
    
    # é€™è£¡æœƒåŠ å…¥å¯¦éš›çš„ ETL é‚è¼¯
    # 1. å¾ MongoDB è®€å–åŸå§‹è³‡æ–™
    # 2. è™•ç†å’Œæ¸…æ´—
    # 3. è¼‰å…¥åˆ° Supabase
    
    print("âœ… ETL Pipeline æ¸¬è©¦å®Œæˆ")
    return "ETL pipeline tested"

def create_test_data():
    """å»ºç«‹æ¸¬è©¦è³‡æ–™"""
    from pymongo import MongoClient
    from pymongo.server_api import ServerApi
    import psycopg2
    from datetime import datetime
    import uuid
    
    try:
        # 1. åœ¨ MongoDB Atlas å»ºç«‹æ¸¬è©¦è³‡æ–™
        mongodb_url = os.getenv('MONGODB_ATLAS_URL')
        if mongodb_url:
            client = MongoClient(mongodb_url, server_api=ServerApi('1'))
            db = client[os.getenv('MONGODB_ATLAS_DB_NAME', 'job_market_data')]
            
            test_job = {
                "source": "linkedin",
                "job_data": {
                    "job_id": f"local_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    "job_title": "Senior Data Engineer - Local Test",
                    "company": "Local Dev Corp",
                    "location": "San Francisco, CA",
                    "salary": "$130,000 - $190,000",
                    "description": "Local development test job posting...",
                    "skills": ["Python", "SQL", "Airflow", "Docker", "Supabase"],
                    "employment_type": "Full-time"
                },
                "metadata": {
                    "scraped_at": datetime.utcnow(),
                    "batch_id": f"local_dev_batch_{datetime.now().strftime('%Y%m%d')}",
                    "scraper_version": "1.0.0-local",
                    "source_url": "https://linkedin.com/jobs/local_test"
                }
            }
            
            result = db['raw_jobs_data'].insert_one(test_job)
            print(f"âœ… MongoDB Atlas: æ¸¬è©¦è³‡æ–™å·²æ’å…¥ï¼ŒID: {result.inserted_id}")
            client.close()
        
        # 2. åœ¨ Supabase å»ºç«‹å°æ‡‰çš„è™•ç†å¾Œè³‡æ–™
        supabase_url = os.getenv('SUPABASE_DB_URL')
        if supabase_url:
            conn = psycopg2.connect(supabase_url)
            cur = conn.cursor()
            
            # ç°¡å–®æ¸¬è©¦ï¼šæ’å…¥ä¸€ç­† fact_jobs è³‡æ–™
            # æ³¨æ„ï¼šå¯¦éš›ä¸Šéœ€è¦å…ˆæœ‰å°æ‡‰çš„ç¶­åº¦è¡¨è³‡æ–™
            print("âœ… Supabase: æ¸¬è©¦è³‡æ–™è™•ç†é‚è¼¯æº–å‚™å°±ç·’")
            
            conn.close()
    
    except Exception as e:
        print(f"âŒ å»ºç«‹æ¸¬è©¦è³‡æ–™å¤±æ•—: {str(e)}")
        raise

# DAG å®šç¾©
default_args = {
    'owner': 'local-dev',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'local_development_test',
    default_args=default_args,
    description='æœ¬åœ°é–‹ç™¼ç’°å¢ƒ - é›²ç«¯è³‡æ–™åº«é€£ç·šæ¸¬è©¦',
    schedule=None,  # æ‰‹å‹•è§¸ç™¼
    catchup=False,
    tags=['local', 'development', 'cloud-database']
)

# ä»»å‹™å®šç¾©
test_db_task = PythonOperator(
    task_id='test_cloud_databases',
    python_callable=test_cloud_databases,
    dag=dag
)

create_data_task = PythonOperator(
    task_id='create_test_data',
    python_callable=create_test_data,
    dag=dag
)

test_etl_task = PythonOperator(
    task_id='test_etl_pipeline',
    python_callable=test_etl_pipeline,
    dag=dag
)

system_info_task = BashOperator(
    task_id='system_info',
    bash_command='''
    echo "ğŸ–¥ï¸ æœ¬åœ°é–‹ç™¼ç’°å¢ƒè³‡è¨Š:"
    echo "Python: $(python --version)"
    echo "ç•¶å‰æ™‚é–“: $(date)"
    echo "å·¥ä½œç›®éŒ„: $(pwd)"
    echo "Docker ç‹€æ…‹:"
    docker compose ps || echo "Docker Compose æœªé‹è¡Œ"
    ''',
    dag=dag
)

# ä»»å‹™ä¾è³´
system_info_task >> test_db_task >> create_data_task >> test_etl_task