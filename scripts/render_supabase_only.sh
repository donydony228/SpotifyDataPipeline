#!/bin/bash
# scripts/render_supabase_only.sh
# Render éƒ¨ç½² - å¼·åˆ¶ä½¿ç”¨ Supabaseï¼Œä¸æä¾› SQLite å‚™ç”¨æ–¹æ¡ˆ

echo "ğŸ¯ Render + Supabase å¼·åˆ¶é€£ç·šç‰ˆæœ¬"

# 1. æ›´æ–°å•Ÿå‹•è…³æœ¬ - åªæ”¯æ´ Supabase
cat > scripts/render_start.sh << 'EOF'
#!/bin/bash
echo "ğŸš€ Render + Supabase éƒ¨ç½²"
echo "========================="

export AIRFLOW_HOME=/app/airflow_home

# æª¢æŸ¥å¿…è¦ç’°å¢ƒè®Šæ•¸
echo "ğŸ” æª¢æŸ¥ç’°å¢ƒè®Šæ•¸..."
if [ -z "$SUPABASE_DB_URL" ]; then
    echo "âŒ éŒ¯èª¤ï¼šSUPABASE_DB_URL ç’°å¢ƒè®Šæ•¸æœªè¨­å®š"
    echo "è«‹åœ¨ Render Dashboard è¨­å®šæ­¤ç’°å¢ƒè®Šæ•¸"
    exit 1
fi

echo "âœ… SUPABASE_DB_URL å·²è¨­å®š"

# æ¸¬è©¦ Supabase é€£ç·š - å¿…é ˆæˆåŠŸ
echo "ğŸ”— æ¸¬è©¦ Supabase é€£ç·š..."
python3 -c "
import os
import psycopg2
import sys
from urllib.parse import urlparse

def test_supabase_connection():
    supabase_url = os.getenv('SUPABASE_DB_URL')
    
    try:
        print(f'ğŸ” é€£ç·šåˆ° Supabase...')
        
        # è§£æ URL é¡¯ç¤ºåŸºæœ¬è³‡è¨Šï¼ˆéš±è—å¯†ç¢¼ï¼‰
        parsed = urlparse(supabase_url)
        masked_url = f'postgresql://{parsed.username}:***@{parsed.hostname}:{parsed.port}{parsed.path}'
        print(f'ğŸ“ ç›®æ¨™: {masked_url}')
        
        # å˜—è©¦é€£ç·šï¼Œå¢åŠ è¶…æ™‚æ™‚é–“
        conn = psycopg2.connect(
            supabase_url,
            connect_timeout=30,
            application_name='render-airflow'
        )
        
        # æ¸¬è©¦æŸ¥è©¢
        cur = conn.cursor()
        cur.execute('SELECT version();')
        version = cur.fetchone()[0]
        print(f'âœ… PostgreSQL ç‰ˆæœ¬: {version[:50]}...')
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æˆ‘å€‘çš„ schema
        cur.execute(\"\"\"
            SELECT schema_name 
            FROM information_schema.schemata 
            WHERE schema_name IN ('dwh', 'raw_staging', 'clean_staging', 'business_staging')
        \"\"\")
        schemas = [row[0] for row in cur.fetchall()]
        if schemas:
            print(f'âœ… æ‰¾åˆ°è³‡æ–™åº« Schema: {schemas}')
        else:
            print('âš ï¸ æœªæ‰¾åˆ°å°ˆæ¡ˆ Schemaï¼Œä½†é€£ç·šæ­£å¸¸')
        
        conn.close()
        print('âœ… Supabase é€£ç·šæ¸¬è©¦æˆåŠŸï¼')
        return True
        
    except psycopg2.OperationalError as e:
        print(f'âŒ Supabase é€£ç·šå¤±æ•— (OperationalError): {e}')
        return False
    except Exception as e:
        print(f'âŒ Supabase é€£ç·šå¤±æ•—: {e}')
        return False

if not test_supabase_connection():
    print('')
    print('ğŸš¨ Supabase é€£ç·šå¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ')
    print('è«‹æª¢æŸ¥ï¼š')
    print('1. SUPABASE_DB_URL æ ¼å¼æ˜¯å¦æ­£ç¢º')
    print('2. Supabase å°ˆæ¡ˆæ˜¯å¦æ­£å¸¸é‹è¡Œ')
    print('3. ç¶²è·¯é€£ç·šæ˜¯å¦ç©©å®š')
    print('4. è³‡æ–™åº«å¯†ç¢¼æ˜¯å¦æ­£ç¢º')
    sys.exit(1)
"

echo "ğŸ“Š ä½¿ç”¨ Supabase PostgreSQL ä½œç‚ºå”¯ä¸€è³‡æ–™åº«"

# å»ºç«‹ Airflow é…ç½®
echo "âš™ï¸ å»ºç«‹ Airflow é…ç½®..."
mkdir -p $AIRFLOW_HOME

cat > $AIRFLOW_HOME/airflow.cfg << EOC
[core]
dags_folder = /app/dags
base_log_folder = /app/logs
logging_level = INFO
executor = LocalExecutor
load_examples = False
fernet_key = ${AIRFLOW__CORE__FERNET_KEY:-render-fernet-key-32-chars-long!!}

[database]
sql_alchemy_conn = $SUPABASE_DB_URL

[webserver]
web_server_port = 8080
secret_key = ${AIRFLOW__WEBSERVER__SECRET_KEY:-render-secret-key}
expose_config = True
base_url = http://0.0.0.0:8080

[api]
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

[scheduler]
catchup_by_default = False

[logging]
logging_level = INFO
remote_logging = False
base_log_folder = /app/logs
EOC

# åˆå§‹åŒ– Airflow è³‡æ–™åº«
echo "ğŸ—ƒï¸ åˆå§‹åŒ– Airflow è³‡æ–™åº«..."
echo "ğŸ“Š ä½¿ç”¨ Supabase PostgreSQL"

# å…ˆå˜—è©¦é·ç§»ï¼Œå¤±æ•—å‰‡åˆå§‹åŒ–
if airflow db migrate; then
    echo "âœ… è³‡æ–™åº«é·ç§»æˆåŠŸ"
else
    echo "âš ï¸ é·ç§»å¤±æ•—ï¼ŒåŸ·è¡Œåˆå§‹åŒ–..."
    airflow db init
fi

# å»ºç«‹ç®¡ç†å“¡ç”¨æˆ¶
echo "ğŸ‘¤ å»ºç«‹ç®¡ç†å“¡ç”¨æˆ¶..."
airflow users create \
    --username admin \
    --firstname Render \
    --lastname Admin \
    --role Admin \
    --email admin@render.com \
    --password admin123 \
    --verbose || echo "â„¹ï¸ ç”¨æˆ¶å¯èƒ½å·²å­˜åœ¨"

# å•Ÿå‹• Airflow Scheduler (èƒŒæ™¯åŸ·è¡Œ)
echo "ğŸ“… å•Ÿå‹• Airflow Scheduler..."
airflow scheduler &
SCHEDULER_PID=$!

# ç­‰å¾… Scheduler å•Ÿå‹•
echo "â³ ç­‰å¾… Scheduler å•Ÿå‹•..."
sleep 20

# å•Ÿå‹• Airflow Webserver (å‰æ™¯åŸ·è¡Œ)
echo "ğŸŒ å•Ÿå‹• Airflow Webserver..."
echo "ğŸ‰ Render + Supabase éƒ¨ç½²å®Œæˆï¼"
echo "ğŸ“ å­˜å– URL: https://ä½ çš„æ‡‰ç”¨å.onrender.com"
echo "ğŸ‘¤ ç™»å…¥å¸³è™Ÿ: admin / admin123"
echo "ğŸ“Š è³‡æ–™åº«: Supabase PostgreSQL"

# å‰æ™¯åŸ·è¡Œ webserverï¼Œè®“å®¹å™¨ä¿æŒé‹è¡Œ
exec airflow webserver --port 8080 --hostname 0.0.0.0
EOF

chmod +x scripts/render_start.sh

# 2. æ›´æ–° Dockerfile - ç§»é™¤ä¸å¿…è¦çš„è¤‡é›œåº¦
cat > Dockerfile << 'EOF'
FROM python:3.9-slim

WORKDIR /app

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£ Python ä¾è³´
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼æª”æ¡ˆ
COPY dags/ ./dags/
COPY src/ ./src/
COPY config/ ./config/
COPY sql/ ./sql/
COPY scripts/ ./scripts/

# å»ºç«‹å¿…è¦ç›®éŒ„
RUN mkdir -p /app/logs /app/data /app/airflow_home

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV AIRFLOW_HOME=/app/airflow_home
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False
ENV AIRFLOW__CORE__EXECUTOR=LocalExecutor
ENV AIRFLOW__LOGGING__LOGGING_LEVEL=INFO

# è¤‡è£½å•Ÿå‹•è…³æœ¬
COPY scripts/render_start.sh /app/start.sh
RUN chmod +x /app/start.sh

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¥åº·æª¢æŸ¥ - æª¢æŸ¥ Airflow Webserver
HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=5 \
  CMD curl -f http://localhost:8080/health || exit 1

# å•Ÿå‹•å‘½ä»¤
CMD ["/app/start.sh"]
EOF

# 3. æ›´æ–°ç’°å¢ƒè®Šæ•¸èªªæ˜
cat > render_environment_variables.txt << 'ENVEOF'
# Render ç’°å¢ƒè®Šæ•¸è¨­å®š - Supabase å°ˆç”¨ç‰ˆæœ¬
# åœ¨ Render Dashboard > Environment é é¢è¨­å®šé€™äº›è®Šæ•¸

# ===== å¿…è¦è¨­å®š =====

# Airflow æ ¸å¿ƒè¨­å®š
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
AIRFLOW__CORE__FERNET_KEY=render-fernet-key-32-chars-long!!
AIRFLOW__WEBSERVER__SECRET_KEY=render-secret-key

# ===== è³‡æ–™åº«é€£ç·š (å¿…é ˆè¨­å®š) =====

# Supabase PostgreSQL (å¿…é ˆè¨­å®šï¼Œå¦å‰‡å•Ÿå‹•å¤±æ•—)
SUPABASE_DB_URL=postgresql://postgres:[ä½ çš„å¯†ç¢¼]@db.xxx.supabase.co:5432/postgres

# MongoDB Atlas (å¯é¸)
MONGODB_ATLAS_URL=mongodb+srv://[ç”¨æˆ¶å]:[å¯†ç¢¼]@xxx.mongodb.net/?retryWrites=true&w=majority
MONGODB_ATLAS_DB_NAME=job_market_data

# ===== éƒ¨ç½²æ¨™è¨˜ =====
ENVIRONMENT=production
DEPLOYMENT_PLATFORM=render

# ===== é‡è¦èªªæ˜ =====
# 1. SUPABASE_DB_URL æ˜¯å¿…é ˆçš„ï¼Œæ²’æœ‰é€™å€‹è®Šæ•¸å•Ÿå‹•æœƒå¤±æ•—
# 2. è«‹å¾ä½ çš„æœ¬åœ° .env æª”æ¡ˆè¤‡è£½æ­£ç¢ºçš„é€£ç·šå­—ä¸²
# 3. ç¢ºä¿ Supabase å°ˆæ¡ˆæ­£å¸¸é‹è¡Œ
# 4. å¦‚æœé€£ç·šå¤±æ•—ï¼Œæª¢æŸ¥ Render éƒ¨ç½²æ—¥èªŒ
ENVEOF

echo ""
echo "âœ… Render + Supabase å¼·åˆ¶é€£ç·šç‰ˆæœ¬æº–å‚™å®Œæˆï¼"
echo "============================================="
echo ""
echo "ğŸ¯ ä¸»è¦è®Šæ›´ï¼š"
echo "  âœ… ç§»é™¤ SQLite å‚™ç”¨æ–¹æ¡ˆ"
echo "  âœ… å¼·åˆ¶è¦æ±‚ SUPABASE_DB_URL ç’°å¢ƒè®Šæ•¸"
echo "  âœ… è©³ç´°çš„é€£ç·šæ¸¬è©¦å’ŒéŒ¯èª¤è¨Šæ¯"
echo "  âœ… å•Ÿå‹•å‰å¿…é ˆç¢ºèª Supabase é€£ç·šæˆåŠŸ"
echo ""
echo "ğŸš€ ä¸‹ä¸€æ­¥ï¼š"
echo "  1. git add ."
echo "  2. git commit -m 'Force Supabase connection, remove SQLite fallback'"
echo "  3. git push origin main"
echo "  4. åœ¨ Render é‡æ–°éƒ¨ç½²"
echo "  5. ç¢ºä¿ SUPABASE_DB_URL ç’°å¢ƒè®Šæ•¸æ­£ç¢ºè¨­å®š"
echo ""
echo "âš ï¸ é‡è¦ï¼š"
echo "  - å¿…é ˆåœ¨ Render è¨­å®šæ­£ç¢ºçš„ SUPABASE_DB_URL"
echo "  - å¦‚æœ Supabase é€£ç·šå¤±æ•—ï¼Œå®¹å™¨æœƒç«‹å³é€€å‡º"
echo "  - é€™æ¨£ç¢ºä¿åªæœ‰åœ¨ Supabase æ­£å¸¸æ™‚æ‰å•Ÿå‹•æœå‹™"