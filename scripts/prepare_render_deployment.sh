#!/bin/bash
# scripts/prepare_render_deployment.sh
# ç¾åœ‹æ±‚è·å¸‚å ´è³‡æ–™å·¥ç¨‹å°ˆæ¡ˆ - Render éƒ¨ç½²æº–å‚™

echo "ğŸš€ æº–å‚™ Render éƒ¨ç½²..."
echo "=========================="

# 1. å»ºç«‹ç°¡åŒ–çš„ Dockerfile (ç§»é™¤ Railway ç‰¹å®šçš„ä¿®å¾©é‚è¼¯)
echo "ğŸ“¦ å»ºç«‹ Render å°ˆç”¨ Dockerfile..."

cat > Dockerfile << 'EOF'
FROM python:3.9-slim

WORKDIR /app

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£ Python ä¾è³´
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼æª”æ¡ˆ
COPY dags/ ./dags/
COPY src/ ./src/
COPY config/ ./config/
COPY sql/ ./sql/
COPY scripts/ ./scripts/

# å»ºç«‹å¿…è¦ç›®éŒ„
RUN mkdir -p /app/logs /app/data /app/airflow_home

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV AIRFLOW_HOME=/app/airflow_home
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False
ENV AIRFLOW__CORE__EXECUTOR=LocalExecutor
ENV AIRFLOW__LOGGING__LOGGING_LEVEL=INFO

# è¤‡è£½å•Ÿå‹•è…³æœ¬
COPY scripts/render_start.sh /app/start.sh
RUN chmod +x /app/start.sh

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¥åº·æª¢æŸ¥ (Render å‹å–„)
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
  CMD curl -f http://localhost:8080/health || curl -f http://localhost:8080/ || exit 1

# å•Ÿå‹•å‘½ä»¤
CMD ["/app/start.sh"]
EOF

# 2. å»ºç«‹ Render å°ˆç”¨å•Ÿå‹•è…³æœ¬ (ç°¡åŒ–ç‰ˆæœ¬)
echo "ğŸ”§ å»ºç«‹ Render å•Ÿå‹•è…³æœ¬..."

cat > scripts/render_start.sh << 'EOF'
#!/bin/bash
echo "ğŸš€ Render Deployment - US Job Data Engineering Platform"

export AIRFLOW_HOME=/app/airflow_home

# æ¸¬è©¦ Supabase é€£ç·š
echo "ğŸ”— æ¸¬è©¦è³‡æ–™åº«é€£ç·š..."
python3 -c "
import os
import psycopg2

try:
    supabase_url = os.getenv('SUPABASE_DB_URL')
    if supabase_url:
        conn = psycopg2.connect(supabase_url)
        conn.close()
        print('âœ… Supabase é€£ç·šæˆåŠŸ')
        with open('/tmp/db_status.txt', 'w') as f:
            f.write('supabase_ok')
    else:
        print('âš ï¸ SUPABASE_DB_URL æœªè¨­å®š')
        with open('/tmp/db_status.txt', 'w') as f:
            f.write('sqlite_fallback')
except Exception as e:
    print(f'âŒ Supabase é€£ç·šå¤±æ•—: {e}')
    print('ğŸ“ ä½¿ç”¨ SQLite å‚™ç”¨')
    with open('/tmp/db_status.txt', 'w') as f:
        f.write('sqlite_fallback')
"

# æ ¹æ“šé€£ç·šæ¸¬è©¦çµæœè¨­å®šè³‡æ–™åº«
DB_STATUS=$(cat /tmp/db_status.txt 2>/dev/null || echo "sqlite_fallback")

if [ "$DB_STATUS" = "supabase_ok" ]; then
    echo "ğŸ“Š ä½¿ç”¨ Supabase PostgreSQL"
    DB_URL="$SUPABASE_DB_URL"
else
    echo "ğŸ“ ä½¿ç”¨ SQLite å‚™ç”¨è³‡æ–™åº«"
    DB_URL="sqlite:////app/airflow_home/airflow.db"
fi

# å»ºç«‹ Airflow é…ç½®æª”æ¡ˆ
echo "âš™ï¸ å»ºç«‹ Airflow é…ç½®..."
mkdir -p $AIRFLOW_HOME

cat > $AIRFLOW_HOME/airflow.cfg << EOC
[core]
dags_folder = /app/dags
base_log_folder = /app/logs
logging_level = INFO
executor = LocalExecutor
load_examples = False
fernet_key = ${AIRFLOW__CORE__FERNET_KEY:-render-fernet-key-32-chars-long!!}

[database]
sql_alchemy_conn = $DB_URL

[webserver]
web_server_port = 8080
secret_key = ${AIRFLOW__WEBSERVER__SECRET_KEY:-render-secret-key}
expose_config = True
base_url = http://0.0.0.0:8080

[api]
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

[scheduler]
catchup_by_default = False

[logging]
logging_level = INFO
remote_logging = False
base_log_folder = /app/logs
EOC

# åˆå§‹åŒ– Airflow è³‡æ–™åº«
echo "ğŸ—ƒï¸ åˆå§‹åŒ– Airflow è³‡æ–™åº«..."
if [[ "$DB_URL" == *"postgresql"* ]]; then
    echo "ğŸ“Š ä½¿ç”¨ PostgreSQLï¼Œå˜—è©¦é·ç§»..."
    airflow db migrate || airflow db init
else
    echo "ğŸ“ ä½¿ç”¨ SQLiteï¼ŒåŸ·è¡Œåˆå§‹åŒ–..."
    airflow db init
fi

# å»ºç«‹ç®¡ç†å“¡ç”¨æˆ¶
echo "ğŸ‘¤ å»ºç«‹ç®¡ç†å“¡ç”¨æˆ¶..."
airflow users create \
    --username admin \
    --firstname Render \
    --lastname Admin \
    --role Admin \
    --email admin@render.com \
    --password admin123 \
    --verbose || echo "â„¹ï¸ ç”¨æˆ¶å¯èƒ½å·²å­˜åœ¨"

# å•Ÿå‹• Airflow Scheduler (èƒŒæ™¯åŸ·è¡Œ)
echo "ğŸ“… å•Ÿå‹• Airflow Scheduler..."
airflow scheduler &
SCHEDULER_PID=$!

# ç­‰å¾…ä¸€æ®µæ™‚é–“è®“ Scheduler å•Ÿå‹•
sleep 15

# å•Ÿå‹• Airflow Webserver (å‰æ™¯åŸ·è¡Œ)
echo "ğŸŒ å•Ÿå‹• Airflow Webserver..."
echo "ğŸ‰ Render éƒ¨ç½²å®Œæˆï¼å­˜å– URL: https://ä½ çš„æ‡‰ç”¨å.onrender.com"
echo "ğŸ‘¤ ç™»å…¥å¸³è™Ÿ: admin / admin123"

# å‰æ™¯åŸ·è¡Œ webserverï¼Œè®“å®¹å™¨ä¿æŒé‹è¡Œ
exec airflow webserver --port 8080 --hostname 0.0.0.0
EOF

chmod +x scripts/render_start.sh

# 3. å»ºç«‹ Render ç’°å¢ƒè®Šæ•¸æª”æ¡ˆ
echo "ğŸ“ æº–å‚™ç’°å¢ƒè®Šæ•¸..."

cat > render_environment_variables.txt << 'ENVEOF'
# Render ç’°å¢ƒè®Šæ•¸è¨­å®š
# åœ¨ Render Dashboard > Environment é é¢è¨­å®šé€™äº›è®Šæ•¸

# Airflow æ ¸å¿ƒè¨­å®š
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
AIRFLOW__CORE__FERNET_KEY=render-fernet-key-32-chars-long!!
AIRFLOW__WEBSERVER__SECRET_KEY=render-secret-key

# éƒ¨ç½²æ¨™è¨˜
ENVIRONMENT=production
DEPLOYMENT_PLATFORM=render

# é›²ç«¯è³‡æ–™åº«é€£ç·š (è«‹å¾ .env æª”æ¡ˆè¤‡è£½å¯¦éš›å€¼)
ENVEOF

# å¾ .env æª”æ¡ˆå–å¾—å¯¦éš›çš„é›²ç«¯é€£ç·šè³‡è¨Š
if [ -f ".env" ]; then
    echo "ğŸ” å¾ .env å–å¾—é›²ç«¯é€£ç·šè³‡è¨Š..."
    
    # åŠ å…¥ Supabase è¨­å®š
    if grep -q "SUPABASE_DB_URL" .env; then
        echo "SUPABASE_DB_URL=$(grep SUPABASE_DB_URL .env | cut -d'=' -f2-)" >> render_environment_variables.txt
    fi
    
    # åŠ å…¥ MongoDB è¨­å®š
    if grep -q "MONGODB_ATLAS_URL" .env; then
        echo "MONGODB_ATLAS_URL=$(grep MONGODB_ATLAS_URL .env | cut -d'=' -f2-)" >> render_environment_variables.txt
    fi
    
    if grep -q "MONGODB_ATLAS_DB_NAME" .env; then
        echo "MONGODB_ATLAS_DB_NAME=$(grep MONGODB_ATLAS_DB_NAME .env | cut -d'=' -f2-)" >> render_environment_variables.txt
    fi
    
    echo "âœ… é›²ç«¯é€£ç·šè³‡è¨Šå·²åŠ å…¥"
else
    echo "âš ï¸ .env æª”æ¡ˆä¸å­˜åœ¨ï¼Œè«‹æ‰‹å‹•åŠ å…¥é›²ç«¯é€£ç·šè³‡è¨Š"
fi

# 4. å»ºç«‹ Render éƒ¨ç½²æª¢æŸ¥æ¸…å–®
echo "ğŸ“‹ å»ºç«‹éƒ¨ç½²æª¢æŸ¥æ¸…å–®..."

cat > RENDER_DEPLOYMENT_CHECKLIST.md << 'EOF'
# ğŸš€ Render éƒ¨ç½²æª¢æŸ¥æ¸…å–®

## âœ… æº–å‚™éšæ®µ

### æœ¬åœ°æº–å‚™
- [ ] ç¨‹å¼ç¢¼å·²æ¨é€åˆ° GitHub
- [ ] Dockerfile å·²æ›´æ–°ç‚º Render ç‰ˆæœ¬
- [ ] å•Ÿå‹•è…³æœ¬å·²å»ºç«‹ (scripts/render_start.sh)
- [ ] ç’°å¢ƒè®Šæ•¸æª”æ¡ˆå·²æº–å‚™ (render_environment_variables.txt)

### é›²ç«¯è³‡æ–™åº«ç¢ºèª
- [ ] Supabase PostgreSQL æ­£å¸¸é‹è¡Œ
- [ ] MongoDB Atlas æ­£å¸¸é‹è¡Œ
- [ ] æœ¬åœ°å¯ä»¥æ­£å¸¸é€£æ¥é›²ç«¯è³‡æ–™åº«

## ğŸŒ Render éƒ¨ç½²æ­¥é©Ÿ

### 1. å»ºç«‹ Render æœå‹™
1. å‰å¾€ https://render.com
2. é»æ“Š "New +" â†’ "Web Service"
3. é€£æ¥ä½ çš„ GitHub repository
4. é¸æ“‡ repository: `us-job-market-data-engineering`

### 2. åŸºæœ¬è¨­å®š
- **Name**: `us-job-data-platform` (æˆ–ä½ å–œæ­¡çš„åç¨±)
- **Environment**: `Docker`
- **Region**: `Oregon (US West)` æˆ– `Ohio (US East)`
- **Branch**: `main`

### 3. é€²éšè¨­å®š
- **Dockerfile Path**: `./Dockerfile` (é è¨­å€¼)
- **Auto-Deploy**: `Yes`

### 4. ç’°å¢ƒè®Šæ•¸è¨­å®š
åœ¨ "Environment" é é¢åŠ å…¥ä»¥ä¸‹è®Šæ•¸ï¼š

```
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
AIRFLOW__CORE__FERNET_KEY=render-fernet-key-32-chars-long!!
AIRFLOW__WEBSERVER__SECRET_KEY=render-secret-key
ENVIRONMENT=production
DEPLOYMENT_PLATFORM=render

# å¾ render_environment_variables.txt è¤‡è£½ Supabase å’Œ MongoDB è¨­å®š
SUPABASE_DB_URL=postgresql://postgres:[password]@db.xxx.supabase.co:5432/postgres
MONGODB_ATLAS_URL=mongodb+srv://...
MONGODB_ATLAS_DB_NAME=job_market_data
```

### 5. éƒ¨ç½²
- é»æ“Š "Create Web Service"
- ç­‰å¾…å»ºç½®å’Œéƒ¨ç½²å®Œæˆ (ç´„ 5-10 åˆ†é˜)

## ğŸ” éƒ¨ç½²å¾Œé©—è­‰

### æª¢æŸ¥éƒ¨ç½²ç‹€æ…‹
- [ ] éƒ¨ç½²æˆåŠŸ (ç¶ è‰²ç‹€æ…‹)
- [ ] æœå‹™æ­£åœ¨é‹è¡Œ
- [ ] æ²’æœ‰éŒ¯èª¤æ—¥èªŒ

### æ¸¬è©¦ Airflow
- [ ] èƒ½å¤ å­˜å– Airflow UI (`https://ä½ çš„æ‡‰ç”¨å.onrender.com`)
- [ ] èƒ½å¤ ç™»å…¥ (admin / admin123)
- [ ] å¯ä»¥çœ‹åˆ° DAGs åˆ—è¡¨
- [ ] hello_world_dag å­˜åœ¨ä¸¦å¯åŸ·è¡Œ

### æ¸¬è©¦è³‡æ–™åº«é€£ç·š
- [ ] æª¢æŸ¥æ—¥èªŒä¸­çš„è³‡æ–™åº«é€£ç·šç‹€æ…‹
- [ ] å¦‚æœä½¿ç”¨ Supabaseï¼Œç¢ºèªé€£ç·šæˆåŠŸ
- [ ] å¦‚æœé™ç´šåˆ° SQLiteï¼Œç¢ºèªæ­£å¸¸é‹ä½œ

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ
1. **éƒ¨ç½²å¤±æ•—**
   - æª¢æŸ¥ Dockerfile èªæ³•
   - ç¢ºèªæ‰€æœ‰æª”æ¡ˆéƒ½å·²æ¨é€åˆ° GitHub

2. **æœå‹™ç„¡æ³•å•Ÿå‹•**
   - æª¢æŸ¥ Render æ—¥èªŒ
   - ç¢ºèªç’°å¢ƒè®Šæ•¸è¨­å®šæ­£ç¢º

3. **ç„¡æ³•å­˜å– Airflow UI**
   - ç¢ºèªæœå‹™æ­£åœ¨é‹è¡Œ
   - æª¢æŸ¥å¥åº·æª¢æŸ¥ç‹€æ…‹

4. **è³‡æ–™åº«é€£ç·šå¤±æ•—**
   - ç¢ºèª Supabase URL æ­£ç¢º
   - æª¢æŸ¥ç¶²è·¯é€£ç·šæ—¥èªŒ
   - ç¢ºèªæœƒè‡ªå‹•é™ç´šåˆ° SQLite

### é æœŸçµæœ
âœ… **æˆåŠŸæ¨™æº–**ï¼š
- Render éƒ¨ç½²æˆåŠŸ
- Airflow UI å¯æ­£å¸¸å­˜å–
- ç®¡ç†å“¡å¸³è™Ÿå¯æ­£å¸¸ç™»å…¥
- DAGs å¯æ­£å¸¸é¡¯ç¤ºå’ŒåŸ·è¡Œ
- è³‡æ–™åº«é€£ç·šæ­£å¸¸ (Supabase æˆ– SQLite)

## ğŸ“ ä¸‹ä¸€æ­¥
éƒ¨ç½²æˆåŠŸå¾Œï¼š
1. æ¸¬è©¦ç¾æœ‰ DAGs
2. é–‹ç™¼ç¬¬ä¸€å€‹çˆ¬èŸ²
3. è¨­å®šç›£æ§å’Œå‘Šè­¦
4. é€æ­¥å®Œå–„ ETL Pipeline
EOF

# 5. æ¸¬è©¦é€£ç·š (å¯é¸)
echo "ğŸ§ª æ¸¬è©¦é›²ç«¯è³‡æ–™åº«é€£ç·š..."
if [ -f ".env" ]; then
    python3 scripts/test_supabase_connection.py 2>/dev/null && echo "âœ… Supabase é€£ç·šæ­£å¸¸"
    python3 scripts/test_mongodb_atlas.py 2>/dev/null && echo "âœ… MongoDB Atlas é€£ç·šæ­£å¸¸"
fi

echo ""
echo "ğŸ‰ Render éƒ¨ç½²æº–å‚™å®Œæˆï¼"
echo "=========================="
echo ""
echo "ğŸ“ å»ºç«‹çš„æª”æ¡ˆï¼š"
echo "  âœ… Dockerfile (Render å„ªåŒ–ç‰ˆ)"
echo "  âœ… scripts/render_start.sh"
echo "  âœ… render_environment_variables.txt"
echo "  âœ… RENDER_DEPLOYMENT_CHECKLIST.md"
echo ""
echo "ğŸš€ ä¸‹ä¸€æ­¥ï¼š"
echo "  1. æª¢æŸ¥æª”æ¡ˆå…§å®¹"
echo "  2. git add ."
echo "  3. git commit -m 'Prepare Render deployment'"
echo "  4. git push origin main"
echo "  5. å‰å¾€ https://render.com å»ºç«‹æœå‹™"
echo ""
echo "ğŸ’¡ é‡é»ï¼š"
echo "  - ç›¸æ¯” Railwayï¼Œç§»é™¤äº†è¤‡é›œçš„ IPv6 ä¿®å¾©é‚è¼¯"
echo "  - æ™ºæ…§è³‡æ–™åº«é€£ç·šï¼šå„ªå…ˆ Supabaseï¼Œå¤±æ•—å‰‡ç”¨ SQLite"
echo "  - ç°¡åŒ–çš„å¥åº·æª¢æŸ¥æ©Ÿåˆ¶"
echo "  - Render åŸç”Ÿæ”¯æ´ PostgreSQL å¤–éƒ¨é€£ç·š"